{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "from typing import List, Dict\n",
    "import codecs\n",
    "import torch\n",
    "import sys\n",
    "import myutils\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myl = [i for i in range(0,11)]\n",
    "myl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    ents = []\n",
    "    curEnts = []\n",
    "    with open(path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                ents.append(curEnts)\n",
    "                curEnts = []\n",
    "            elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                curEnts.append(line.split('\\t')[1])\n",
    "    return ents\n",
    "\n",
    "def read_labels(path):\n",
    "    ents = []\n",
    "    curEnts = []\n",
    "    with open(path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                ents.append(curEnts)\n",
    "                curEnts = []\n",
    "            elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                curEnts.append(line.split('\\t')[2])\n",
    "    return ents\n",
    "\n",
    "def read_index(path):\n",
    "    ents = []\n",
    "    curEnts = []\n",
    "    with open(path, encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                ents.append(curEnts)\n",
    "                curEnts = []\n",
    "            elif line[0] == '#' and len(line.split('\\t')) == 1:\n",
    "                continue\n",
    "            else:\n",
    "                curEnts.append(line.split('\\t')[0])\n",
    "    return ents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data(\"en_ewt-ud-train.iob2\")\n",
    "train_labels = read_labels(\"en_ewt-ud-train.iob2\")\n",
    "train_index = read_index(\"en_ewt-ud-train.iob2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Where', 'in', 'the', 'world', 'is', 'Iguazu', '?'],\n",
       " ['Iguazu', 'Falls'],\n",
       " ['Widely',\n",
       "  'considered',\n",
       "  'to',\n",
       "  'be',\n",
       "  'one',\n",
       "  'of',\n",
       "  'the',\n",
       "  'most',\n",
       "  'spectacular',\n",
       "  'waterfalls',\n",
       "  'in',\n",
       "  'the',\n",
       "  'world',\n",
       "  ',',\n",
       "  'the',\n",
       "  'Iguazu',\n",
       "  'Falls',\n",
       "  'on',\n",
       "  'the',\n",
       "  'border',\n",
       "  'of',\n",
       "  'Argentina',\n",
       "  'and',\n",
       "  'Brazil',\n",
       "  ',',\n",
       "  'are',\n",
       "  'a',\n",
       "  'certainly',\n",
       "  'must',\n",
       "  'see',\n",
       "  'attraction',\n",
       "  'in',\n",
       "  'the',\n",
       "  'area',\n",
       "  '.']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list of unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "\n",
    "for labels in train_labels:\n",
    "    for label in labels:\n",
    "        if label not in label_list:\n",
    "            label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-LOC', 'I-LOC', 'B-PER', 'B-ORG', 'I-ORG', 'I-PER']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\obe\\.cache\\huggingface\\hub\\models--distilbert--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(data, label_list):\n",
    "    tokenized_inputs = tokenizer(data, truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(label_list):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "tokenized_train = tokenize_and_align_labels(train_data, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_labels = tokenized_train[\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-100, 'O', 'O', 'O', 'O', 'O', 'B-LOC', -100, -100, 'O', -100]\n",
      "['[CLS]', 'where', 'in', 'the', 'world', 'is', 'i', '##gua', '##zu', '?', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_labels[0])\n",
    "print(tokenized_train[0].tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-LOC\",\n",
    "    2: \"I-LOC\",\n",
    "    3: \"B-PER\",\n",
    "    4: \"B-ORG\",\n",
    "    5: \"I-ORG\",\n",
    "    6: \"I-PER\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    'B-LOC': 1,\n",
    "    'I-LOC': 2,\n",
    "    'B-PER': 3,\n",
    "    'B-ORG': 4,\n",
    "    'I-ORG': 5,\n",
    "    'I-PER': 6\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copying from the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wnut = load_dataset(\"wnut_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['@paulwalk',\n",
       "  'It',\n",
       "  \"'s\",\n",
       "  'the',\n",
       "  'view',\n",
       "  'from',\n",
       "  'where',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'living',\n",
       "  'for',\n",
       "  'two',\n",
       "  'weeks',\n",
       "  '.',\n",
       "  'Empire',\n",
       "  'State',\n",
       "  'Building',\n",
       "  '=',\n",
       "  'ESB',\n",
       "  '.',\n",
       "  'Pretty',\n",
       "  'bad',\n",
       "  'storm',\n",
       "  'here',\n",
       "  'last',\n",
       "  'evening',\n",
       "  '.'],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-corporation',\n",
       " 'I-corporation',\n",
       " 'B-creative-work',\n",
       " 'I-creative-work',\n",
       " 'B-group',\n",
       " 'I-group',\n",
       " 'B-location',\n",
       " 'I-location',\n",
       " 'B-person',\n",
       " 'I-person',\n",
       " 'B-product',\n",
       " 'I-product']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " '@',\n",
       " 'paul',\n",
       " '##walk',\n",
       " 'it',\n",
       " \"'\",\n",
       " 's',\n",
       " 'the',\n",
       " 'view',\n",
       " 'from',\n",
       " 'where',\n",
       " 'i',\n",
       " \"'\",\n",
       " 'm',\n",
       " 'living',\n",
       " 'for',\n",
       " 'two',\n",
       " 'weeks',\n",
       " '.',\n",
       " 'empire',\n",
       " 'state',\n",
       " 'building',\n",
       " '=',\n",
       " 'es',\n",
       " '##b',\n",
       " '.',\n",
       " 'pretty',\n",
       " 'bad',\n",
       " 'storm',\n",
       " 'here',\n",
       " 'last',\n",
       " 'evening',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = wnut[\"train\"][0]\n",
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/3394 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3394/3394 [00:00<00:00, 5320.18 examples/s]\n",
      "Map: 100%|██████████| 1009/1009 [00:00<00:00, 7637.40 examples/s]\n",
      "Map: 100%|██████████| 1287/1287 [00:00<00:00, 6749.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_wnut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['@paulwalk',\n",
       "  'It',\n",
       "  \"'s\",\n",
       "  'the',\n",
       "  'view',\n",
       "  'from',\n",
       "  'where',\n",
       "  'I',\n",
       "  \"'m\",\n",
       "  'living',\n",
       "  'for',\n",
       "  'two',\n",
       "  'weeks',\n",
       "  '.',\n",
       "  'Empire',\n",
       "  'State',\n",
       "  'Building',\n",
       "  '=',\n",
       "  'ESB',\n",
       "  '.',\n",
       "  'Pretty',\n",
       "  'bad',\n",
       "  'storm',\n",
       "  'here',\n",
       "  'last',\n",
       "  'evening',\n",
       "  '.'],\n",
       " 'ner_tags': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  7,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'input_ids': [101,\n",
       "  1030,\n",
       "  2703,\n",
       "  17122,\n",
       "  2009,\n",
       "  1005,\n",
       "  1055,\n",
       "  1996,\n",
       "  3193,\n",
       "  2013,\n",
       "  2073,\n",
       "  1045,\n",
       "  1005,\n",
       "  1049,\n",
       "  2542,\n",
       "  2005,\n",
       "  2048,\n",
       "  3134,\n",
       "  1012,\n",
       "  3400,\n",
       "  2110,\n",
       "  2311,\n",
       "  1027,\n",
       "  9686,\n",
       "  2497,\n",
       "  1012,\n",
       "  3492,\n",
       "  2919,\n",
       "  4040,\n",
       "  2182,\n",
       "  2197,\n",
       "  3944,\n",
       "  1012,\n",
       "  102],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  0,\n",
       "  -100,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  7,\n",
       "  8,\n",
       "  8,\n",
       "  0,\n",
       "  7,\n",
       "  -100,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  -100]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_wnut[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "labels = [label_list[i] for i in example[f\"ner_tags\"]]\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-corporation\",\n",
    "    2: \"I-corporation\",\n",
    "    3: \"B-creative-work\",\n",
    "    4: \"I-creative-work\",\n",
    "    5: \"B-group\",\n",
    "    6: \"I-group\",\n",
    "    7: \"B-location\",\n",
    "    8: \"I-location\",\n",
    "    9: \"B-person\",\n",
    "    10: \"I-person\",\n",
    "    11: \"B-product\",\n",
    "    12: \"I-product\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-corporation\": 1,\n",
    "    \"I-corporation\": 2,\n",
    "    \"B-creative-work\": 3,\n",
    "    \"I-creative-work\": 4,\n",
    "    \"B-group\": 5,\n",
    "    \"I-group\": 6,\n",
    "    \"B-location\": 7,\n",
    "    \"I-location\": 8,\n",
    "    \"B-person\": 9,\n",
    "    \"I-person\": 10,\n",
    "    \"B-product\": 11,\n",
    "    \"I-product\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.16.1-cp311-cp311-win_amd64.whl.metadata (3.5 kB)\n",
      "Collecting tensorflow-intel==2.16.1 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.5.26 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading flatbuffers-24.3.7-py2.py3-none-any.whl.metadata (849 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading gast-0.5.4-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.10.0)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading grpcio-1.62.1-cp311-cp311-win_amd64.whl.metadata (4.2 kB)\n",
      "Collecting tensorboard<2.17,>=2.16 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading tensorboard-2.16.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.0.5)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.26.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: rich in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.7)\n",
      "Requirement already satisfied: dm-tree in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2023.11.17)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading werkzeug-3.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\obe\\appdata\\roaming\\python\\python311\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.16.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n",
      "Downloading tensorflow-2.16.1-cp311-cp311-win_amd64.whl (2.1 kB)\n",
      "Downloading tensorflow_intel-2.16.1-cp311-cp311-win_amd64.whl (377.0 MB)\n",
      "   ---------------------------------------- 0.0/377.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.2/377.0 MB 7.4 MB/s eta 0:00:52\n",
      "   ---------------------------------------- 0.5/377.0 MB 8.3 MB/s eta 0:00:46\n",
      "   ---------------------------------------- 0.8/377.0 MB 7.4 MB/s eta 0:00:51\n",
      "   ---------------------------------------- 1.2/377.0 MB 7.3 MB/s eta 0:00:52\n",
      "   ---------------------------------------- 1.4/377.0 MB 7.0 MB/s eta 0:00:54\n",
      "   ---------------------------------------- 1.8/377.0 MB 7.7 MB/s eta 0:00:49\n",
      "   ---------------------------------------- 2.1/377.0 MB 7.4 MB/s eta 0:00:51\n",
      "   ---------------------------------------- 2.5/377.0 MB 7.8 MB/s eta 0:00:48\n",
      "   ---------------------------------------- 2.8/377.0 MB 7.5 MB/s eta 0:00:50\n",
      "   ---------------------------------------- 3.2/377.0 MB 7.8 MB/s eta 0:00:48\n",
      "   ---------------------------------------- 3.6/377.0 MB 7.9 MB/s eta 0:00:48\n",
      "   ---------------------------------------- 3.9/377.0 MB 7.8 MB/s eta 0:00:48\n",
      "   ---------------------------------------- 4.3/377.0 MB 7.9 MB/s eta 0:00:48\n",
      "   ---------------------------------------- 4.7/377.0 MB 7.9 MB/s eta 0:00:48\n",
      "    --------------------------------------- 5.1/377.0 MB 8.1 MB/s eta 0:00:46\n",
      "    --------------------------------------- 5.4/377.0 MB 8.0 MB/s eta 0:00:47\n",
      "    --------------------------------------- 5.8/377.0 MB 8.2 MB/s eta 0:00:46\n",
      "    --------------------------------------- 6.2/377.0 MB 8.0 MB/s eta 0:00:47\n",
      "    --------------------------------------- 6.6/377.0 MB 8.0 MB/s eta 0:00:47\n",
      "    --------------------------------------- 6.9/377.0 MB 8.2 MB/s eta 0:00:46\n",
      "    --------------------------------------- 7.2/377.0 MB 8.1 MB/s eta 0:00:46\n",
      "    --------------------------------------- 7.6/377.0 MB 8.2 MB/s eta 0:00:45\n",
      "    --------------------------------------- 8.0/377.0 MB 8.2 MB/s eta 0:00:45\n",
      "    --------------------------------------- 8.3/377.0 MB 8.1 MB/s eta 0:00:46\n",
      "    --------------------------------------- 8.7/377.0 MB 8.1 MB/s eta 0:00:46\n",
      "    --------------------------------------- 9.0/377.0 MB 8.2 MB/s eta 0:00:45\n",
      "    --------------------------------------- 9.4/377.0 MB 8.2 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 9.7/377.0 MB 8.2 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 10.1/377.0 MB 8.2 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 10.5/377.0 MB 8.2 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 10.8/377.0 MB 8.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 11.2/377.0 MB 8.3 MB/s eta 0:00:45\n",
      "   - -------------------------------------- 11.6/377.0 MB 8.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 11.9/377.0 MB 8.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 12.3/377.0 MB 8.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 12.6/377.0 MB 8.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 13.0/377.0 MB 8.5 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 13.4/377.0 MB 8.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 13.7/377.0 MB 8.5 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 14.1/377.0 MB 8.5 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 14.4/377.0 MB 8.5 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 14.8/377.0 MB 8.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 15.2/377.0 MB 8.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 15.5/377.0 MB 8.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 15.9/377.0 MB 8.4 MB/s eta 0:00:44\n",
      "   - -------------------------------------- 16.2/377.0 MB 8.4 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 16.7/377.0 MB 8.5 MB/s eta 0:00:43\n",
      "   - -------------------------------------- 17.2/377.0 MB 8.6 MB/s eta 0:00:42\n",
      "   - -------------------------------------- 17.8/377.0 MB 8.7 MB/s eta 0:00:42\n",
      "   - -------------------------------------- 18.3/377.0 MB 9.0 MB/s eta 0:00:40\n",
      "   -- ------------------------------------- 18.9/377.0 MB 9.1 MB/s eta 0:00:40\n",
      "   -- ------------------------------------- 19.5/377.0 MB 9.2 MB/s eta 0:00:39\n",
      "   -- ------------------------------------- 20.2/377.0 MB 9.6 MB/s eta 0:00:38\n",
      "   -- ------------------------------------- 20.7/377.0 MB 9.9 MB/s eta 0:00:36\n",
      "   -- ------------------------------------- 21.3/377.0 MB 10.1 MB/s eta 0:00:36\n",
      "   -- ------------------------------------- 21.9/377.0 MB 10.4 MB/s eta 0:00:35\n",
      "   -- ------------------------------------- 22.3/377.0 MB 10.6 MB/s eta 0:00:34\n",
      "   -- ------------------------------------- 23.0/377.0 MB 10.9 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 23.6/377.0 MB 11.3 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 23.9/377.0 MB 11.1 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 24.3/377.0 MB 11.3 MB/s eta 0:00:32\n",
      "   -- ------------------------------------- 25.0/377.0 MB 11.7 MB/s eta 0:00:31\n",
      "   -- ------------------------------------- 25.6/377.0 MB 12.1 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 26.3/377.0 MB 12.6 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 27.0/377.0 MB 13.1 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 27.5/377.0 MB 12.8 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 28.0/377.0 MB 12.8 MB/s eta 0:00:28\n",
      "   --- ------------------------------------ 28.6/377.0 MB 13.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 29.2/377.0 MB 13.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 29.7/377.0 MB 13.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 30.3/377.0 MB 13.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 30.8/377.0 MB 13.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 31.3/377.0 MB 12.8 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 31.9/377.0 MB 13.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 32.5/377.0 MB 13.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 33.0/377.0 MB 13.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 33.7/377.0 MB 13.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 34.3/377.0 MB 13.4 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 34.9/377.0 MB 13.6 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 35.4/377.0 MB 13.6 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 36.0/377.0 MB 13.6 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 36.6/377.0 MB 13.4 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 37.1/377.0 MB 13.4 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 37.6/377.0 MB 13.4 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 38.2/377.0 MB 13.6 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 38.8/377.0 MB 13.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 39.2/377.0 MB 13.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 39.8/377.0 MB 13.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 40.4/377.0 MB 13.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 41.0/377.0 MB 13.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 41.7/377.0 MB 13.4 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 42.2/377.0 MB 13.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 42.8/377.0 MB 13.6 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 43.4/377.0 MB 13.4 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 44.0/377.0 MB 13.4 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 44.6/377.0 MB 13.6 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 45.0/377.0 MB 13.4 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 45.5/377.0 MB 13.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 46.0/377.0 MB 13.1 MB/s eta 0:00:26\n",
      "   ---- ----------------------------------- 46.6/377.0 MB 13.1 MB/s eta 0:00:26\n",
      "   ----- ---------------------------------- 47.3/377.0 MB 13.4 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 47.9/377.0 MB 13.6 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 48.5/377.0 MB 13.6 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 49.0/377.0 MB 13.6 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 49.7/377.0 MB 13.9 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 50.4/377.0 MB 13.9 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 51.2/377.0 MB 14.2 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 51.8/377.0 MB 14.2 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 52.4/377.0 MB 14.2 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 53.1/377.0 MB 14.6 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 53.7/377.0 MB 14.2 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 54.2/377.0 MB 14.2 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 54.9/377.0 MB 14.6 MB/s eta 0:00:23\n",
      "   ----- ---------------------------------- 55.6/377.0 MB 14.9 MB/s eta 0:00:22\n",
      "   ----- ---------------------------------- 56.2/377.0 MB 14.9 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 56.8/377.0 MB 14.9 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 57.4/377.0 MB 14.6 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 58.0/377.0 MB 14.9 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 58.6/377.0 MB 14.9 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 59.3/377.0 MB 14.9 MB/s eta 0:00:22\n",
      "   ------ --------------------------------- 60.0/377.0 MB 15.2 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 60.7/377.0 MB 15.2 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 61.5/377.0 MB 15.2 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 62.3/377.0 MB 15.2 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 63.1/377.0 MB 15.6 MB/s eta 0:00:21\n",
      "   ------ --------------------------------- 63.9/377.0 MB 16.0 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 64.8/377.0 MB 16.4 MB/s eta 0:00:20\n",
      "   ------ --------------------------------- 65.7/377.0 MB 16.8 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 66.5/377.0 MB 17.2 MB/s eta 0:00:19\n",
      "   ------- -------------------------------- 67.1/377.0 MB 17.3 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 68.0/377.0 MB 18.2 MB/s eta 0:00:18\n",
      "   ------- -------------------------------- 68.8/377.0 MB 18.2 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 69.6/377.0 MB 18.2 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 70.5/377.0 MB 18.7 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 71.3/377.0 MB 18.2 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 72.1/377.0 MB 18.7 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 72.9/377.0 MB 18.7 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 73.6/377.0 MB 18.2 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 74.4/377.0 MB 18.7 MB/s eta 0:00:17\n",
      "   ------- -------------------------------- 75.3/377.0 MB 18.7 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 76.1/377.0 MB 18.7 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 77.0/377.0 MB 18.7 MB/s eta 0:00:17\n",
      "   -------- ------------------------------- 77.9/377.0 MB 19.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 78.6/377.0 MB 18.7 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 79.5/377.0 MB 19.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 80.5/377.0 MB 19.8 MB/s eta 0:00:15\n",
      "   -------- ------------------------------- 81.1/377.0 MB 19.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 81.9/377.0 MB 19.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 82.9/377.0 MB 19.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 83.5/377.0 MB 19.3 MB/s eta 0:00:16\n",
      "   -------- ------------------------------- 84.4/377.0 MB 19.2 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 85.3/377.0 MB 19.2 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 86.1/377.0 MB 18.7 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 86.9/377.0 MB 19.2 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 87.5/377.0 MB 18.7 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 88.2/377.0 MB 18.7 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 89.1/377.0 MB 18.7 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 89.9/377.0 MB 18.7 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 90.5/377.0 MB 18.7 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 91.2/377.0 MB 18.2 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 92.0/377.0 MB 18.2 MB/s eta 0:00:16\n",
      "   --------- ------------------------------ 92.8/377.0 MB 17.7 MB/s eta 0:00:17\n",
      "   --------- ------------------------------ 93.5/377.0 MB 18.2 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 94.4/377.0 MB 17.7 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 95.0/377.0 MB 17.7 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 95.8/377.0 MB 17.7 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 96.2/377.0 MB 17.7 MB/s eta 0:00:16\n",
      "   ---------- ----------------------------- 96.8/377.0 MB 17.3 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 97.4/377.0 MB 16.8 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 97.9/377.0 MB 16.8 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 98.8/377.0 MB 16.4 MB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 99.5/377.0 MB 16.4 MB/s eta 0:00:17\n",
      "   ---------- ---------------------------- 100.4/377.0 MB 16.4 MB/s eta 0:00:17\n",
      "   ---------- ---------------------------- 101.1/377.0 MB 16.8 MB/s eta 0:00:17\n",
      "   ---------- ---------------------------- 101.9/377.0 MB 17.3 MB/s eta 0:00:16\n",
      "   ---------- ---------------------------- 102.2/377.0 MB 16.8 MB/s eta 0:00:17\n",
      "   ---------- ---------------------------- 103.0/377.0 MB 16.4 MB/s eta 0:00:17\n",
      "   ---------- ---------------------------- 103.9/377.0 MB 16.4 MB/s eta 0:00:17\n",
      "   ---------- ---------------------------- 104.7/377.0 MB 16.8 MB/s eta 0:00:17\n",
      "   ---------- ---------------------------- 105.4/377.0 MB 16.4 MB/s eta 0:00:17\n",
      "   ---------- ---------------------------- 106.3/377.0 MB 16.4 MB/s eta 0:00:17\n",
      "   ----------- --------------------------- 106.9/377.0 MB 16.8 MB/s eta 0:00:17\n",
      "   ----------- --------------------------- 107.7/377.0 MB 17.2 MB/s eta 0:00:16\n",
      "   ----------- --------------------------- 108.3/377.0 MB 17.7 MB/s eta 0:00:16\n",
      "   ----------- --------------------------- 109.0/377.0 MB 17.2 MB/s eta 0:00:16\n",
      "   ----------- --------------------------- 109.7/377.0 MB 17.7 MB/s eta 0:00:16\n",
      "   ----------- --------------------------- 110.6/377.0 MB 17.2 MB/s eta 0:00:16\n",
      "   ----------- --------------------------- 111.4/377.0 MB 17.2 MB/s eta 0:00:16\n",
      "   ----------- --------------------------- 112.2/377.0 MB 17.2 MB/s eta 0:00:16\n",
      "   ----------- --------------------------- 112.8/377.0 MB 18.2 MB/s eta 0:00:15\n",
      "   ----------- --------------------------- 113.5/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ----------- --------------------------- 114.1/377.0 MB 17.2 MB/s eta 0:00:16\n",
      "   ----------- --------------------------- 114.9/377.0 MB 17.2 MB/s eta 0:00:16\n",
      "   ----------- --------------------------- 115.7/377.0 MB 17.2 MB/s eta 0:00:16\n",
      "   ------------ -------------------------- 116.5/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 117.2/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 118.1/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 118.9/377.0 MB 18.2 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 119.5/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 120.2/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 120.9/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 121.7/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 122.4/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------ -------------------------- 123.2/377.0 MB 18.2 MB/s eta 0:00:14\n",
      "   ------------ -------------------------- 123.9/377.0 MB 18.2 MB/s eta 0:00:14\n",
      "   ------------ -------------------------- 124.5/377.0 MB 18.2 MB/s eta 0:00:14\n",
      "   ------------ -------------------------- 125.3/377.0 MB 18.2 MB/s eta 0:00:14\n",
      "   ------------- ------------------------- 125.9/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------- ------------------------- 126.8/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------- ------------------------- 127.4/377.0 MB 17.2 MB/s eta 0:00:15\n",
      "   ------------- ------------------------- 128.3/377.0 MB 17.7 MB/s eta 0:00:15\n",
      "   ------------- ------------------------- 128.8/377.0 MB 17.2 MB/s eta 0:00:15\n",
      "   ------------- ------------------------- 129.8/377.0 MB 17.7 MB/s eta 0:00:14\n",
      "   ------------- ------------------------- 130.7/377.0 MB 18.2 MB/s eta 0:00:14\n",
      "   ------------- ------------------------- 131.5/377.0 MB 17.7 MB/s eta 0:00:14\n",
      "   ------------- ------------------------- 132.4/377.0 MB 18.2 MB/s eta 0:00:14\n",
      "   ------------- ------------------------- 133.1/377.0 MB 18.2 MB/s eta 0:00:14\n",
      "   ------------- ------------------------- 133.9/377.0 MB 17.7 MB/s eta 0:00:14\n",
      "   ------------- ------------------------- 134.5/377.0 MB 18.7 MB/s eta 0:00:13\n",
      "   -------------- ------------------------ 135.5/377.0 MB 18.2 MB/s eta 0:00:14\n",
      "   -------------- ------------------------ 136.0/377.0 MB 17.7 MB/s eta 0:00:14\n",
      "   -------------- ------------------------ 136.9/377.0 MB 18.7 MB/s eta 0:00:13\n",
      "   -------------- ------------------------ 137.9/377.0 MB 19.2 MB/s eta 0:00:13\n",
      "   -------------- ------------------------ 138.7/377.0 MB 18.7 MB/s eta 0:00:13\n",
      "   -------------- ------------------------ 139.6/377.0 MB 19.2 MB/s eta 0:00:13\n",
      "   -------------- ------------------------ 140.5/377.0 MB 19.3 MB/s eta 0:00:13\n",
      "   -------------- ------------------------ 141.1/377.0 MB 19.3 MB/s eta 0:00:13\n",
      "   -------------- ------------------------ 141.9/377.0 MB 18.7 MB/s eta 0:00:13\n",
      "   -------------- ------------------------ 142.6/377.0 MB 18.2 MB/s eta 0:00:13\n",
      "   -------------- ------------------------ 143.2/377.0 MB 18.7 MB/s eta 0:00:13\n",
      "   -------------- ------------------------ 144.0/377.0 MB 18.2 MB/s eta 0:00:13\n",
      "   -------------- ------------------------ 144.7/377.0 MB 18.2 MB/s eta 0:00:13\n",
      "   --------------- ----------------------- 145.6/377.0 MB 18.7 MB/s eta 0:00:13\n",
      "   --------------- ----------------------- 146.4/377.0 MB 18.7 MB/s eta 0:00:13\n",
      "   --------------- ----------------------- 147.1/377.0 MB 18.2 MB/s eta 0:00:13\n",
      "   --------------- ----------------------- 147.9/377.0 MB 18.7 MB/s eta 0:00:13\n",
      "   --------------- ----------------------- 148.8/377.0 MB 18.7 MB/s eta 0:00:13\n",
      "   --------------- ----------------------- 149.4/377.0 MB 17.7 MB/s eta 0:00:13\n",
      "   --------------- ----------------------- 150.5/377.0 MB 18.7 MB/s eta 0:00:13\n",
      "   --------------- ----------------------- 151.4/377.0 MB 18.7 MB/s eta 0:00:13\n",
      "   --------------- ----------------------- 152.3/377.0 MB 18.7 MB/s eta 0:00:13\n",
      "   --------------- ----------------------- 152.9/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   --------------- ----------------------- 153.6/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   --------------- ----------------------- 154.4/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   ---------------- ---------------------- 155.1/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   ---------------- ---------------------- 155.6/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   ---------------- ---------------------- 156.5/377.0 MB 18.2 MB/s eta 0:00:13\n",
      "   ---------------- ---------------------- 157.3/377.0 MB 18.2 MB/s eta 0:00:13\n",
      "   ---------------- ---------------------- 158.2/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   ---------------- ---------------------- 159.0/377.0 MB 19.9 MB/s eta 0:00:11\n",
      "   ---------------- ---------------------- 159.9/377.0 MB 19.3 MB/s eta 0:00:12\n",
      "   ---------------- ---------------------- 160.5/377.0 MB 19.3 MB/s eta 0:00:12\n",
      "   ---------------- ---------------------- 161.1/377.0 MB 18.2 MB/s eta 0:00:12\n",
      "   ---------------- ---------------------- 161.9/377.0 MB 18.2 MB/s eta 0:00:12\n",
      "   ---------------- ---------------------- 162.8/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   ---------------- ---------------------- 163.7/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 164.5/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 165.2/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 166.2/377.0 MB 19.9 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 167.1/377.0 MB 19.8 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 168.0/377.0 MB 19.3 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 168.6/377.0 MB 19.3 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 169.3/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 170.1/377.0 MB 18.7 MB/s eta 0:00:12\n",
      "   ----------------- --------------------- 171.1/377.0 MB 18.7 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 171.6/377.0 MB 18.7 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 172.3/377.0 MB 18.7 MB/s eta 0:00:11\n",
      "   ----------------- --------------------- 173.1/377.0 MB 17.7 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 174.0/377.0 MB 18.7 MB/s eta 0:00:11\n",
      "   ------------------ -------------------- 174.9/377.0 MB 18.7 MB/s eta 0:00:11\n",
      "   ------------------ -------------------- 175.5/377.0 MB 19.3 MB/s eta 0:00:11\n",
      "   ------------------ -------------------- 176.4/377.0 MB 18.2 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 176.9/377.0 MB 18.7 MB/s eta 0:00:11\n",
      "   ------------------ -------------------- 177.7/377.0 MB 18.2 MB/s eta 0:00:11\n",
      "   ------------------ -------------------- 178.3/377.0 MB 18.2 MB/s eta 0:00:11\n",
      "   ------------------ -------------------- 178.4/377.0 MB 16.8 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 179.1/377.0 MB 16.8 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 179.6/377.0 MB 16.0 MB/s eta 0:00:13\n",
      "   ------------------ -------------------- 180.7/377.0 MB 16.8 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 181.5/377.0 MB 16.4 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 182.4/377.0 MB 16.8 MB/s eta 0:00:12\n",
      "   ------------------ -------------------- 183.4/377.0 MB 17.2 MB/s eta 0:00:12\n",
      "   ------------------- ------------------- 184.3/377.0 MB 16.8 MB/s eta 0:00:12\n",
      "   ------------------- ------------------- 184.6/377.0 MB 16.4 MB/s eta 0:00:12\n",
      "   ------------------- ------------------- 185.8/377.0 MB 16.8 MB/s eta 0:00:12\n",
      "   ------------------- ------------------- 186.5/377.0 MB 16.4 MB/s eta 0:00:12\n",
      "   ------------------- ------------------- 187.6/377.0 MB 17.2 MB/s eta 0:00:11\n",
      "   ------------------- ------------------- 188.7/377.0 MB 19.3 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 189.6/377.0 MB 20.5 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 190.5/377.0 MB 20.5 MB/s eta 0:00:10\n",
      "   ------------------- ------------------- 191.5/377.0 MB 21.8 MB/s eta 0:00:09\n",
      "   ------------------- ------------------- 192.7/377.0 MB 22.6 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 193.7/377.0 MB 21.8 MB/s eta 0:00:09\n",
      "   -------------------- ------------------ 194.7/377.0 MB 23.4 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 195.4/377.0 MB 24.3 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 196.7/377.0 MB 24.2 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 197.3/377.0 MB 23.4 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 198.8/377.0 MB 25.2 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 199.6/377.0 MB 25.2 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 200.7/377.0 MB 26.2 MB/s eta 0:00:07\n",
      "   -------------------- ------------------ 201.6/377.0 MB 24.2 MB/s eta 0:00:08\n",
      "   -------------------- ------------------ 202.6/377.0 MB 24.2 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 203.6/377.0 MB 25.2 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 204.4/377.0 MB 23.4 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 205.4/377.0 MB 23.4 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 206.4/377.0 MB 24.2 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 207.2/377.0 MB 24.3 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 208.3/377.0 MB 24.2 MB/s eta 0:00:07\n",
      "   --------------------- ----------------- 209.4/377.0 MB 23.4 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 210.3/377.0 MB 23.4 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 211.3/377.0 MB 23.4 MB/s eta 0:00:08\n",
      "   --------------------- ----------------- 212.5/377.0 MB 24.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 213.7/377.0 MB 23.4 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 214.9/377.0 MB 23.4 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 215.7/377.0 MB 24.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 216.9/377.0 MB 24.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 217.7/377.0 MB 23.4 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 218.8/377.0 MB 23.4 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 219.7/377.0 MB 24.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 220.8/377.0 MB 24.2 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 221.4/377.0 MB 23.4 MB/s eta 0:00:07\n",
      "   ---------------------- ---------------- 222.1/377.0 MB 22.6 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 222.9/377.0 MB 21.8 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 223.9/377.0 MB 22.6 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 224.6/377.0 MB 22.6 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 225.6/377.0 MB 21.1 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 226.7/377.0 MB 21.8 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 227.6/377.0 MB 21.8 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 228.6/377.0 MB 21.1 MB/s eta 0:00:08\n",
      "   ----------------------- --------------- 229.5/377.0 MB 21.8 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 230.4/377.0 MB 21.8 MB/s eta 0:00:07\n",
      "   ----------------------- --------------- 231.3/377.0 MB 21.1 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 232.5/377.0 MB 21.9 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 233.2/377.0 MB 22.6 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 234.0/377.0 MB 21.8 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 235.2/377.0 MB 22.6 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 236.0/377.0 MB 22.6 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 236.7/377.0 MB 21.8 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 237.4/377.0 MB 21.1 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 238.3/377.0 MB 21.1 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 238.7/377.0 MB 21.1 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 240.2/377.0 MB 21.1 MB/s eta 0:00:07\n",
      "   ------------------------ -------------- 241.0/377.0 MB 21.1 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 242.0/377.0 MB 21.1 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 242.9/377.0 MB 21.1 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 243.9/377.0 MB 21.1 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 244.9/377.0 MB 21.8 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 245.8/377.0 MB 21.1 MB/s eta 0:00:07\n",
      "   ------------------------- ------------- 246.8/377.0 MB 21.9 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 247.9/377.0 MB 21.8 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 248.8/377.0 MB 21.8 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 249.9/377.0 MB 23.4 MB/s eta 0:00:06\n",
      "   ------------------------- ------------- 250.7/377.0 MB 22.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 251.7/377.0 MB 23.4 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 252.8/377.0 MB 22.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 253.8/377.0 MB 22.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 254.4/377.0 MB 21.1 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 255.9/377.0 MB 21.8 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 256.9/377.0 MB 23.4 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 257.9/377.0 MB 22.6 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 258.8/377.0 MB 22.5 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 259.8/377.0 MB 23.4 MB/s eta 0:00:06\n",
      "   -------------------------- ------------ 260.7/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 261.7/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 262.4/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 263.5/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 264.5/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 265.4/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 266.4/377.0 MB 22.6 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 267.4/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 268.6/377.0 MB 22.6 MB/s eta 0:00:05\n",
      "   --------------------------- ----------- 269.7/377.0 MB 22.6 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 270.8/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 271.7/377.0 MB 22.6 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 272.7/377.0 MB 24.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 273.5/377.0 MB 22.6 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 274.7/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 275.7/377.0 MB 24.2 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 276.8/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 278.0/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 279.0/377.0 MB 22.5 MB/s eta 0:00:05\n",
      "   ---------------------------- ---------- 279.9/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 280.9/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 281.9/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 282.9/377.0 MB 23.4 MB/s eta 0:00:05\n",
      "   ----------------------------- --------- 283.9/377.0 MB 23.4 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 284.8/377.0 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 285.8/377.0 MB 23.4 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 286.8/377.0 MB 23.4 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 287.8/377.0 MB 23.4 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 288.8/377.0 MB 24.2 MB/s eta 0:00:04\n",
      "   ----------------------------- --------- 289.8/377.0 MB 23.4 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 290.8/377.0 MB 23.4 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 291.8/377.0 MB 24.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 292.8/377.0 MB 24.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 293.8/377.0 MB 23.4 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 294.9/377.0 MB 23.4 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 296.1/377.0 MB 24.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 297.1/377.0 MB 24.2 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 298.2/377.0 MB 23.4 MB/s eta 0:00:04\n",
      "   ------------------------------ -------- 299.2/377.0 MB 24.2 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 300.3/377.0 MB 24.2 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 301.2/377.0 MB 23.4 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 302.2/377.0 MB 23.4 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 303.2/377.0 MB 24.2 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 304.2/377.0 MB 23.4 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 305.2/377.0 MB 23.4 MB/s eta 0:00:04\n",
      "   ------------------------------- ------- 306.1/377.0 MB 24.2 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 307.0/377.0 MB 24.3 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 308.1/377.0 MB 24.2 MB/s eta 0:00:03\n",
      "   ------------------------------- ------- 309.1/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 310.0/377.0 MB 24.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 311.0/377.0 MB 24.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 312.0/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 312.9/377.0 MB 24.3 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 313.9/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 314.8/377.0 MB 24.2 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 316.0/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 316.9/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 317.8/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   -------------------------------- ------ 318.9/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 319.9/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 320.8/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 321.6/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 322.6/377.0 MB 22.6 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 323.6/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 324.8/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 325.7/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 326.8/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   --------------------------------- ----- 327.7/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 328.8/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 329.9/377.0 MB 23.4 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 330.8/377.0 MB 22.6 MB/s eta 0:00:03\n",
      "   ---------------------------------- ---- 331.7/377.0 MB 23.4 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 332.3/377.0 MB 22.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 333.3/377.0 MB 22.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 334.2/377.0 MB 22.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 335.2/377.0 MB 22.6 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 335.6/377.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 336.8/377.0 MB 21.8 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 337.7/377.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 338.7/377.0 MB 21.8 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 339.8/377.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 340.8/377.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 341.9/377.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 342.8/377.0 MB 22.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 343.8/377.0 MB 22.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 344.7/377.0 MB 22.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 345.8/377.0 MB 23.4 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 346.7/377.0 MB 22.6 MB/s eta 0:00:02\n",
      "   ----------------------------------- --- 347.8/377.0 MB 23.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 348.7/377.0 MB 23.4 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 349.3/377.0 MB 22.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 350.1/377.0 MB 21.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 351.1/377.0 MB 22.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 351.7/377.0 MB 21.9 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 352.6/377.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 353.6/377.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 354.5/377.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 355.5/377.0 MB 21.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 356.4/377.0 MB 20.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 357.3/377.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 358.2/377.0 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 359.0/377.0 MB 20.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 360.0/377.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 360.9/377.0 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 361.8/377.0 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 362.7/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 363.6/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 364.5/377.0 MB 22.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 365.4/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 366.2/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 367.2/377.0 MB 22.5 MB/s eta 0:00:01\n",
      "   --------------------------------------  368.1/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  369.1/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  370.0/377.0 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  371.1/377.0 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  372.0/377.0 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  372.9/377.0 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  373.7/377.0 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  374.6/377.0 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------------------------  375.6/377.0 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  376.4/377.0 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   --------------------------------------  377.0/377.0 MB 21.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 377.0/377.0 MB 5.8 MB/s eta 0:00:00\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-24.3.7-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "   ---------------------------------------- 0.0/57.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 57.5/57.5 kB ? eta 0:00:00\n",
      "Downloading grpcio-1.62.1-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.8/3.8 MB 24.8 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.7/3.8 MB 21.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.4/3.8 MB 22.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 3.3/3.8 MB 21.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 18.6 MB/s eta 0:00:00\n",
      "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.9/26.4 MB 28.7 MB/s eta 0:00:01\n",
      "   -- ------------------------------------- 1.9/26.4 MB 23.9 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 2.9/26.4 MB 23.0 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 3.8/26.4 MB 22.0 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 4.7/26.4 MB 21.6 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 5.7/26.4 MB 22.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 6.6/26.4 MB 22.1 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 7.5/26.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 8.4/26.4 MB 22.3 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 9.5/26.4 MB 21.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 10.6/26.4 MB 22.6 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 11.5/26.4 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 12.4/26.4 MB 22.6 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 13.2/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 14.1/26.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 15.0/26.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 15.9/26.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 16.8/26.4 MB 21.8 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 17.9/26.4 MB 21.8 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 18.7/26.4 MB 22.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 19.6/26.4 MB 21.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 20.8/26.4 MB 21.8 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 21.8/26.4 MB 22.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 22.9/26.4 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.8/26.4 MB 22.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 24.9/26.4 MB 23.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 25.7/26.4 MB 22.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 21.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 21.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  26.4/26.4 MB 21.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 26.4/26.4 MB 18.2 MB/s eta 0:00:00\n",
      "Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.5/65.5 kB 3.5 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.25.3-cp310-abi3-win_amd64.whl (413 kB)\n",
      "   ---------------------------------------- 0.0/413.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 413.4/413.4 kB 13.0 MB/s eta 0:00:00\n",
      "Downloading tensorboard-2.16.2-py3-none-any.whl (5.5 MB)\n",
      "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.7/5.5 MB 22.8 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.5/5.5 MB 23.5 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 2.2/5.5 MB 20.0 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 3.0/5.5 MB 18.9 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 3.8/5.5 MB 18.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 4.5/5.5 MB 19.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.3/5.5 MB 18.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.5/5.5 MB 18.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.5/5.5 MB 16.7 MB/s eta 0:00:00\n",
      "Downloading tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------------- -------------------- 0.7/1.5 MB 22.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 18.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 13.5 MB/s eta 0:00:00\n",
      "Downloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-1.16.0-cp311-cp311-win_amd64.whl (37 kB)\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "   ---------------------------------------- 0.0/105.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 105.4/105.4 kB 6.3 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
      "   ---------------------------------------- 0.0/226.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 226.7/226.7 kB 13.5 MB/s eta 0:00:00\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, opt-einsum, markdown, grpcio, google-pasta, gast, astunparse, tensorboard, tensorflow-intel, tensorflow\n",
      "Successfully installed astunparse-1.6.3 flatbuffers-24.3.7 gast-0.5.4 google-pasta-0.2.0 grpcio-1.62.1 libclang-18.1.1 markdown-3.6 opt-einsum-3.3.0 protobuf-4.25.3 tensorboard-2.16.2 tensorboard-data-server-0.7.2 tensorflow-2.16.1 tensorflow-intel-2.16.1 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.4.0 werkzeug-3.0.1 wrapt-1.16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "EagerTensor object has no attribute 'size'. \n        If you are looking for numpy-related methods, please run the following:\n        tf.experimental.numpy.experimental_enable_numpy_behavior()\n      ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 24\u001b[0m\n\u001b[0;32m      1\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      2\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_awesome_wnut_model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2e-5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;66;03m#push_to_hub=True,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m )\n\u001b[0;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     15\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     16\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     21\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     22\u001b[0m )\n\u001b[1;32m---> 24\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1624\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1622\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1623\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1624\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1961\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   1960\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1961\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1964\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1965\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1966\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1967\u001b[0m ):\n\u001b[0;32m   1968\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1969\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2902\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2901\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2902\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2904\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   2905\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:2925\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2923\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2924\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2925\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2926\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2927\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2928\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:1233\u001b[0m, in \u001b[0;36mDistilBertForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m \u001b[38;5;124;03m    Labels for computing the token classification loss. Indices should be in `[0, ..., config.num_labels - 1]`.\u001b[39;00m\n\u001b[0;32m   1230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1231\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1233\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1238\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1239\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1241\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1243\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1245\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(sequence_output)\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:803\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n\u001b[1;32m--> 803\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n\u001b[0;32m    804\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    805\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:255\u001b[0m, in \u001b[0;36mTensor.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[0;32m    252\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mT\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mravel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtranspose\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreshape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclip\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    253\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtolist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;66;03m# TODO(wangpeng): Export the enable_numpy_behavior knob\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m    256\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124m      If you are looking for numpy-related methods, please run the following:\u001b[39m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124m      tf.experimental.numpy.experimental_enable_numpy_behavior()\u001b[39m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m    260\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: EagerTensor object has no attribute 'size'. \n        If you are looking for numpy-related methods, please run the following:\n        tf.experimental.numpy.experimental_enable_numpy_behavior()\n      "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_wnut_model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_wnut[\"train\"],\n",
    "    eval_dataset=tokenized_wnut[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
