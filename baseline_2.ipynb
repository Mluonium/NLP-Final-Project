{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data \n",
    "def read_iob2_file(path):\n",
    "    \"\"\"\n",
    "    read in iob2 file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words and labels for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue # skip comments\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[1])\n",
    "            current_tags.append(tok[2])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    # check for last one\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Iguazu', 'Falls'], ['B-LOC', 'I-LOC'])\n"
     ]
    }
   ],
   "source": [
    "train_data = read_iob2_file(\"en_ewt-ud-train.iob2\")\n",
    "dev_data = read_iob2_file(\"en_ewt-ud-dev.iob2\")\n",
    "print(train_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Where in the world is Iguazu ?', ['O', 'O', 'O', 'O', 'O', 'B-LOC', 'O']]\n"
     ]
    }
   ],
   "source": [
    "# formatting the data \n",
    "def list2sequence(data): \n",
    "    formated_data = [[\" \".join(sublist),labels] for sublist, labels in data]\n",
    "    return formated_data\n",
    "formatted_train_data = list2sequence(train_data)\n",
    "print(formatted_train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing the sentences - output of tokenizer: \n",
    "* input_ids are the indices corresponding to each token in the sentence.\n",
    "* attention_mask indicates whether a token should be attended to or not.\n",
    "* token_type_ids identifies which sequence a token belongs to when there is more than one sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['Where', 'in', 'the', 'world', 'is', 'I', '##gua', '##zu', '?'], ['O', 'O', 'O', 'O', 'O', 'B-LOC', 'B-LOC', 'B-LOC', 'O'])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer  = AutoTokenizer.from_pretrained('bert-base-multilingual-cased', use_fast=False)\n",
    "# Tokenize input text and map tokens to token IDs    \n",
    "def tokenize_and_preserve_labels(sentence, text_labels):\n",
    "    tokenized_sentence = []\n",
    "    labels = []\n",
    "\n",
    "    for word, label in zip(sentence, text_labels):\n",
    "        # Tokenize the word and count # of subwords the word is broken into\n",
    "        tokenized_word = tokenizer.tokenize(word)\n",
    "        n_subwords = len(tokenized_word)\n",
    "\n",
    "        # Add the tokenized word to the final tokenized word list\n",
    "        tokenized_sentence.extend(tokenized_word)\n",
    "\n",
    "        # Add the same label to the new list of labels `n_subwords` times\n",
    "        labels.extend([label] * n_subwords)\n",
    "\n",
    "    return tokenized_sentence, labels\n",
    "\n",
    "train_data_formated = []\n",
    "for tuple_train in train_data: \n",
    "    train_data_formated.append((tokenize_and_preserve_labels(tuple_train[0], tuple_train[1])))\n",
    "print(train_data_formated[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification\n",
    "encoded_train = tokenizer(formatted_train_data[0:100], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "BERT_model = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=7)\n",
    "output = BERT_model(**encoded_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 97, 7])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 97, 7])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch import nn\n",
    "sm = nn.Softmax(dim=2)\n",
    "prob = sm(output.logits)\n",
    "prob.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "encoded_train_b = tokenizer(formatted_train_data[0:100], padding=True, truncation=True, return_tensors=\"pt\")\n",
    "BERT_model_b = BertModel.from_pretrained('bert-base-multilingual-cased', num_labels=7)\n",
    "output_b = BERT_model_b(**encoded_train_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0949,  0.1422,  0.0887,  ..., -0.0247, -0.0395, -0.0088],\n",
       "         [ 0.2475,  0.0377,  0.6476,  ...,  0.4834,  0.0135,  0.0452],\n",
       "         [ 0.6987, -0.1668,  0.0979,  ...,  0.9545, -0.7628,  0.3487],\n",
       "         ...,\n",
       "         [-0.0714,  0.0279,  0.2431,  ...,  0.1540, -0.2002, -0.1019],\n",
       "         [-0.0464,  0.0700,  0.3602,  ...,  0.1506, -0.1863, -0.1306],\n",
       "         [ 0.0758,  0.0617,  0.2668,  ...,  0.2415, -0.1267, -0.0832]],\n",
       "\n",
       "        [[-0.0617,  0.1141,  0.0403,  ..., -0.0199,  0.0229, -0.0085],\n",
       "         [-0.2257, -0.0644,  0.6345,  ...,  0.3778, -0.1432, -0.0899],\n",
       "         [-0.3810, -0.0198,  0.3201,  ...,  0.4905, -0.1122, -0.0650],\n",
       "         ...,\n",
       "         [-0.1374, -0.1141,  0.6238,  ...,  0.2174, -0.1438,  0.0055],\n",
       "         [-0.1371, -0.1092,  0.6450,  ...,  0.2202, -0.1369,  0.0205],\n",
       "         [-0.1680, -0.1125,  0.6214,  ...,  0.2050, -0.1790, -0.0188]],\n",
       "\n",
       "        [[-0.2358,  0.0317,  0.1002,  ..., -0.1532, -0.0120,  0.1002],\n",
       "         [ 0.0787, -0.0938,  0.0666,  ...,  0.6720, -0.0556, -0.0549],\n",
       "         [ 0.2707, -1.0151,  0.6655,  ...,  0.7949, -0.3538,  0.1875],\n",
       "         ...,\n",
       "         [-0.4737, -0.0420,  0.2042,  ...,  0.2919, -0.0672, -0.1608],\n",
       "         [-0.6473,  0.1410,  0.2909,  ...,  0.0640,  0.0713, -0.2282],\n",
       "         [-0.4714, -0.0436,  0.2661,  ...,  0.2167, -0.0810, -0.1564]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.3623,  0.0673,  0.0173,  ..., -0.2902,  0.0808,  0.1219],\n",
       "         [ 0.0408, -0.0405,  0.1052,  ...,  0.4628,  0.0281, -0.2581],\n",
       "         [-0.2280,  0.3823,  0.1282,  ...,  0.5533, -0.5909, -0.0236],\n",
       "         ...,\n",
       "         [-0.1051, -0.2389,  0.2619,  ...,  0.0436,  0.0480, -0.1917],\n",
       "         [-0.3331, -0.0785,  0.2709,  ...,  0.3536,  0.1294, -0.2308],\n",
       "         [ 0.0493, -0.1575,  0.8577,  ...,  0.0580,  0.2114, -0.1867]],\n",
       "\n",
       "        [[-0.2719,  0.0700,  0.1246,  ..., -0.0234,  0.0774,  0.0084],\n",
       "         [-0.0418, -0.2655,  0.6435,  ...,  0.5388,  0.0303, -0.2764],\n",
       "         [ 0.0439, -0.1942,  0.9508,  ...,  0.2545,  0.0384, -0.1088],\n",
       "         ...,\n",
       "         [-0.1929,  0.0849,  0.3622,  ...,  0.0247, -0.2074, -0.1573],\n",
       "         [-0.0960,  0.0638,  0.2456,  ...,  0.1090, -0.0723, -0.1986],\n",
       "         [-0.1777,  0.0954,  0.3746,  ...,  0.1302,  0.0256, -0.3474]],\n",
       "\n",
       "        [[-0.2108,  0.0047,  0.1644,  ..., -0.0540,  0.0114,  0.0506],\n",
       "         [ 0.2040, -0.2122,  0.9398,  ...,  0.3653, -0.2007, -0.2278],\n",
       "         [-0.0231, -0.2432,  0.6089,  ..., -0.1394, -0.1154, -0.1316],\n",
       "         ...,\n",
       "         [-0.0746, -0.1925,  0.5525,  ...,  0.1625,  0.0455, -0.2770],\n",
       "         [ 0.0540, -0.2181,  0.5691,  ...,  0.1551,  0.0394, -0.2577],\n",
       "         [-0.0648, -0.0990,  0.4113,  ...,  0.1254, -0.0924, -0.2294]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_b.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 2: Input Shape: torch.Size([32, 64]), Label Shape: torch.Size([32, 64])\n",
      "Batch 3: Input Shape: torch.Size([32, 66]), Label Shape: torch.Size([32, 66])\n",
      "Batch 4: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 5: Input Shape: torch.Size([32, 42]), Label Shape: torch.Size([32, 42])\n",
      "Batch 6: Input Shape: torch.Size([32, 59]), Label Shape: torch.Size([32, 59])\n",
      "Batch 7: Input Shape: torch.Size([32, 32]), Label Shape: torch.Size([32, 32])\n",
      "Batch 8: Input Shape: torch.Size([32, 61]), Label Shape: torch.Size([32, 61])\n",
      "Batch 9: Input Shape: torch.Size([32, 64]), Label Shape: torch.Size([32, 64])\n",
      "Batch 10: Input Shape: torch.Size([32, 43]), Label Shape: torch.Size([32, 43])\n",
      "Batch 11: Input Shape: torch.Size([32, 51]), Label Shape: torch.Size([32, 51])\n",
      "Batch 12: Input Shape: torch.Size([32, 83]), Label Shape: torch.Size([32, 83])\n",
      "Batch 13: Input Shape: torch.Size([32, 72]), Label Shape: torch.Size([32, 72])\n",
      "Batch 14: Input Shape: torch.Size([32, 40]), Label Shape: torch.Size([32, 40])\n",
      "Batch 15: Input Shape: torch.Size([32, 57]), Label Shape: torch.Size([32, 57])\n",
      "Batch 16: Input Shape: torch.Size([32, 39]), Label Shape: torch.Size([32, 39])\n",
      "Batch 17: Input Shape: torch.Size([32, 79]), Label Shape: torch.Size([32, 79])\n",
      "Batch 18: Input Shape: torch.Size([32, 65]), Label Shape: torch.Size([32, 65])\n",
      "Batch 19: Input Shape: torch.Size([32, 61]), Label Shape: torch.Size([32, 61])\n",
      "Batch 20: Input Shape: torch.Size([32, 43]), Label Shape: torch.Size([32, 43])\n",
      "Batch 21: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 22: Input Shape: torch.Size([32, 84]), Label Shape: torch.Size([32, 84])\n",
      "Batch 23: Input Shape: torch.Size([32, 72]), Label Shape: torch.Size([32, 72])\n",
      "Batch 24: Input Shape: torch.Size([32, 57]), Label Shape: torch.Size([32, 57])\n",
      "Batch 25: Input Shape: torch.Size([32, 66]), Label Shape: torch.Size([32, 66])\n",
      "Batch 26: Input Shape: torch.Size([32, 59]), Label Shape: torch.Size([32, 59])\n",
      "Batch 27: Input Shape: torch.Size([32, 70]), Label Shape: torch.Size([32, 70])\n",
      "Batch 28: Input Shape: torch.Size([32, 55]), Label Shape: torch.Size([32, 55])\n",
      "Batch 29: Input Shape: torch.Size([32, 121]), Label Shape: torch.Size([32, 121])\n",
      "Batch 30: Input Shape: torch.Size([32, 73]), Label Shape: torch.Size([32, 73])\n",
      "Batch 31: Input Shape: torch.Size([32, 63]), Label Shape: torch.Size([32, 63])\n",
      "Batch 32: Input Shape: torch.Size([32, 85]), Label Shape: torch.Size([32, 85])\n",
      "Batch 33: Input Shape: torch.Size([32, 58]), Label Shape: torch.Size([32, 58])\n",
      "Batch 34: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 35: Input Shape: torch.Size([32, 50]), Label Shape: torch.Size([32, 50])\n",
      "Batch 36: Input Shape: torch.Size([32, 76]), Label Shape: torch.Size([32, 76])\n",
      "Batch 37: Input Shape: torch.Size([32, 70]), Label Shape: torch.Size([32, 70])\n",
      "Batch 38: Input Shape: torch.Size([32, 51]), Label Shape: torch.Size([32, 51])\n",
      "Batch 39: Input Shape: torch.Size([32, 44]), Label Shape: torch.Size([32, 44])\n",
      "Batch 40: Input Shape: torch.Size([32, 60]), Label Shape: torch.Size([32, 60])\n",
      "Batch 41: Input Shape: torch.Size([32, 51]), Label Shape: torch.Size([32, 51])\n",
      "Batch 42: Input Shape: torch.Size([32, 57]), Label Shape: torch.Size([32, 57])\n",
      "Batch 43: Input Shape: torch.Size([32, 74]), Label Shape: torch.Size([32, 74])\n",
      "Batch 44: Input Shape: torch.Size([32, 45]), Label Shape: torch.Size([32, 45])\n",
      "Batch 45: Input Shape: torch.Size([32, 47]), Label Shape: torch.Size([32, 47])\n",
      "Batch 46: Input Shape: torch.Size([32, 87]), Label Shape: torch.Size([32, 87])\n",
      "Batch 47: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 48: Input Shape: torch.Size([32, 40]), Label Shape: torch.Size([32, 40])\n",
      "Batch 49: Input Shape: torch.Size([32, 54]), Label Shape: torch.Size([32, 54])\n",
      "Batch 50: Input Shape: torch.Size([32, 47]), Label Shape: torch.Size([32, 47])\n",
      "Batch 51: Input Shape: torch.Size([32, 69]), Label Shape: torch.Size([32, 69])\n",
      "Batch 52: Input Shape: torch.Size([32, 56]), Label Shape: torch.Size([32, 56])\n",
      "Batch 53: Input Shape: torch.Size([32, 46]), Label Shape: torch.Size([32, 46])\n",
      "Batch 54: Input Shape: torch.Size([32, 52]), Label Shape: torch.Size([32, 52])\n",
      "Batch 55: Input Shape: torch.Size([32, 40]), Label Shape: torch.Size([32, 40])\n",
      "Batch 56: Input Shape: torch.Size([32, 113]), Label Shape: torch.Size([32, 113])\n",
      "Batch 57: Input Shape: torch.Size([32, 51]), Label Shape: torch.Size([32, 51])\n",
      "Batch 58: Input Shape: torch.Size([32, 78]), Label Shape: torch.Size([32, 78])\n",
      "Batch 59: Input Shape: torch.Size([32, 42]), Label Shape: torch.Size([32, 42])\n",
      "Batch 60: Input Shape: torch.Size([32, 50]), Label Shape: torch.Size([32, 50])\n",
      "Batch 61: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 62: Input Shape: torch.Size([32, 56]), Label Shape: torch.Size([32, 56])\n",
      "Batch 63: Input Shape: torch.Size([32, 52]), Label Shape: torch.Size([32, 52])\n",
      "Batch 64: Input Shape: torch.Size([32, 72]), Label Shape: torch.Size([32, 72])\n",
      "Batch 65: Input Shape: torch.Size([32, 56]), Label Shape: torch.Size([32, 56])\n",
      "Batch 66: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 67: Input Shape: torch.Size([32, 105]), Label Shape: torch.Size([32, 105])\n",
      "Batch 68: Input Shape: torch.Size([32, 29]), Label Shape: torch.Size([32, 29])\n",
      "Batch 69: Input Shape: torch.Size([32, 47]), Label Shape: torch.Size([32, 47])\n",
      "Batch 70: Input Shape: torch.Size([32, 82]), Label Shape: torch.Size([32, 82])\n",
      "Batch 71: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 72: Input Shape: torch.Size([32, 61]), Label Shape: torch.Size([32, 61])\n",
      "Batch 73: Input Shape: torch.Size([32, 105]), Label Shape: torch.Size([32, 105])\n",
      "Batch 74: Input Shape: torch.Size([32, 57]), Label Shape: torch.Size([32, 57])\n",
      "Batch 75: Input Shape: torch.Size([32, 91]), Label Shape: torch.Size([32, 91])\n",
      "Batch 76: Input Shape: torch.Size([32, 50]), Label Shape: torch.Size([32, 50])\n",
      "Batch 77: Input Shape: torch.Size([32, 69]), Label Shape: torch.Size([32, 69])\n",
      "Batch 78: Input Shape: torch.Size([32, 82]), Label Shape: torch.Size([32, 82])\n",
      "Batch 79: Input Shape: torch.Size([32, 54]), Label Shape: torch.Size([32, 54])\n",
      "Batch 80: Input Shape: torch.Size([32, 70]), Label Shape: torch.Size([32, 70])\n",
      "Batch 81: Input Shape: torch.Size([32, 52]), Label Shape: torch.Size([32, 52])\n",
      "Batch 82: Input Shape: torch.Size([32, 44]), Label Shape: torch.Size([32, 44])\n",
      "Batch 83: Input Shape: torch.Size([32, 72]), Label Shape: torch.Size([32, 72])\n",
      "Batch 84: Input Shape: torch.Size([32, 48]), Label Shape: torch.Size([32, 48])\n",
      "Batch 85: Input Shape: torch.Size([32, 51]), Label Shape: torch.Size([32, 51])\n",
      "Batch 86: Input Shape: torch.Size([32, 65]), Label Shape: torch.Size([32, 65])\n",
      "Batch 87: Input Shape: torch.Size([32, 67]), Label Shape: torch.Size([32, 67])\n",
      "Batch 88: Input Shape: torch.Size([32, 41]), Label Shape: torch.Size([32, 41])\n",
      "Batch 89: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 90: Input Shape: torch.Size([32, 68]), Label Shape: torch.Size([32, 68])\n",
      "Batch 91: Input Shape: torch.Size([32, 64]), Label Shape: torch.Size([32, 64])\n",
      "Batch 92: Input Shape: torch.Size([32, 69]), Label Shape: torch.Size([32, 69])\n",
      "Batch 93: Input Shape: torch.Size([32, 46]), Label Shape: torch.Size([32, 46])\n",
      "Batch 94: Input Shape: torch.Size([32, 78]), Label Shape: torch.Size([32, 78])\n",
      "Batch 95: Input Shape: torch.Size([32, 60]), Label Shape: torch.Size([32, 60])\n",
      "Batch 96: Input Shape: torch.Size([32, 50]), Label Shape: torch.Size([32, 50])\n",
      "Batch 97: Input Shape: torch.Size([32, 158]), Label Shape: torch.Size([32, 158])\n",
      "Batch 98: Input Shape: torch.Size([32, 47]), Label Shape: torch.Size([32, 47])\n",
      "Batch 99: Input Shape: torch.Size([32, 48]), Label Shape: torch.Size([32, 48])\n",
      "Batch 100: Input Shape: torch.Size([32, 41]), Label Shape: torch.Size([32, 41])\n",
      "Batch 101: Input Shape: torch.Size([32, 43]), Label Shape: torch.Size([32, 43])\n",
      "Batch 102: Input Shape: torch.Size([32, 40]), Label Shape: torch.Size([32, 40])\n",
      "Batch 103: Input Shape: torch.Size([32, 82]), Label Shape: torch.Size([32, 82])\n",
      "Batch 104: Input Shape: torch.Size([32, 79]), Label Shape: torch.Size([32, 79])\n",
      "Batch 105: Input Shape: torch.Size([32, 55]), Label Shape: torch.Size([32, 55])\n",
      "Batch 106: Input Shape: torch.Size([32, 63]), Label Shape: torch.Size([32, 63])\n",
      "Batch 107: Input Shape: torch.Size([32, 60]), Label Shape: torch.Size([32, 60])\n",
      "Batch 108: Input Shape: torch.Size([32, 52]), Label Shape: torch.Size([32, 52])\n",
      "Batch 109: Input Shape: torch.Size([32, 78]), Label Shape: torch.Size([32, 78])\n",
      "Batch 110: Input Shape: torch.Size([32, 67]), Label Shape: torch.Size([32, 67])\n",
      "Batch 111: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 112: Input Shape: torch.Size([32, 185]), Label Shape: torch.Size([32, 185])\n",
      "Batch 113: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 114: Input Shape: torch.Size([32, 77]), Label Shape: torch.Size([32, 77])\n",
      "Batch 115: Input Shape: torch.Size([32, 77]), Label Shape: torch.Size([32, 77])\n",
      "Batch 116: Input Shape: torch.Size([32, 46]), Label Shape: torch.Size([32, 46])\n",
      "Batch 117: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 118: Input Shape: torch.Size([32, 57]), Label Shape: torch.Size([32, 57])\n",
      "Batch 119: Input Shape: torch.Size([32, 52]), Label Shape: torch.Size([32, 52])\n",
      "Batch 120: Input Shape: torch.Size([32, 46]), Label Shape: torch.Size([32, 46])\n",
      "Batch 121: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 122: Input Shape: torch.Size([32, 42]), Label Shape: torch.Size([32, 42])\n",
      "Batch 123: Input Shape: torch.Size([32, 63]), Label Shape: torch.Size([32, 63])\n",
      "Batch 124: Input Shape: torch.Size([32, 38]), Label Shape: torch.Size([32, 38])\n",
      "Batch 125: Input Shape: torch.Size([32, 43]), Label Shape: torch.Size([32, 43])\n",
      "Batch 126: Input Shape: torch.Size([32, 58]), Label Shape: torch.Size([32, 58])\n",
      "Batch 127: Input Shape: torch.Size([32, 50]), Label Shape: torch.Size([32, 50])\n",
      "Batch 128: Input Shape: torch.Size([32, 56]), Label Shape: torch.Size([32, 56])\n",
      "Batch 129: Input Shape: torch.Size([32, 58]), Label Shape: torch.Size([32, 58])\n",
      "Batch 130: Input Shape: torch.Size([32, 55]), Label Shape: torch.Size([32, 55])\n",
      "Batch 131: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 132: Input Shape: torch.Size([32, 42]), Label Shape: torch.Size([32, 42])\n",
      "Batch 133: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 134: Input Shape: torch.Size([32, 80]), Label Shape: torch.Size([32, 80])\n",
      "Batch 135: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 136: Input Shape: torch.Size([32, 70]), Label Shape: torch.Size([32, 70])\n",
      "Batch 137: Input Shape: torch.Size([32, 47]), Label Shape: torch.Size([32, 47])\n",
      "Batch 138: Input Shape: torch.Size([32, 71]), Label Shape: torch.Size([32, 71])\n",
      "Batch 139: Input Shape: torch.Size([32, 65]), Label Shape: torch.Size([32, 65])\n",
      "Batch 140: Input Shape: torch.Size([32, 68]), Label Shape: torch.Size([32, 68])\n",
      "Batch 141: Input Shape: torch.Size([32, 60]), Label Shape: torch.Size([32, 60])\n",
      "Batch 142: Input Shape: torch.Size([32, 87]), Label Shape: torch.Size([32, 87])\n",
      "Batch 143: Input Shape: torch.Size([32, 91]), Label Shape: torch.Size([32, 91])\n",
      "Batch 144: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 145: Input Shape: torch.Size([32, 81]), Label Shape: torch.Size([32, 81])\n",
      "Batch 146: Input Shape: torch.Size([32, 73]), Label Shape: torch.Size([32, 73])\n",
      "Batch 147: Input Shape: torch.Size([32, 57]), Label Shape: torch.Size([32, 57])\n",
      "Batch 148: Input Shape: torch.Size([32, 45]), Label Shape: torch.Size([32, 45])\n",
      "Batch 149: Input Shape: torch.Size([32, 79]), Label Shape: torch.Size([32, 79])\n",
      "Batch 150: Input Shape: torch.Size([32, 51]), Label Shape: torch.Size([32, 51])\n",
      "Batch 151: Input Shape: torch.Size([32, 68]), Label Shape: torch.Size([32, 68])\n",
      "Batch 152: Input Shape: torch.Size([32, 66]), Label Shape: torch.Size([32, 66])\n",
      "Batch 153: Input Shape: torch.Size([32, 71]), Label Shape: torch.Size([32, 71])\n",
      "Batch 154: Input Shape: torch.Size([32, 46]), Label Shape: torch.Size([32, 46])\n",
      "Batch 155: Input Shape: torch.Size([32, 76]), Label Shape: torch.Size([32, 76])\n",
      "Batch 156: Input Shape: torch.Size([32, 45]), Label Shape: torch.Size([32, 45])\n",
      "Batch 157: Input Shape: torch.Size([32, 58]), Label Shape: torch.Size([32, 58])\n",
      "Batch 158: Input Shape: torch.Size([32, 60]), Label Shape: torch.Size([32, 60])\n",
      "Batch 159: Input Shape: torch.Size([32, 81]), Label Shape: torch.Size([32, 81])\n",
      "Batch 160: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 161: Input Shape: torch.Size([32, 93]), Label Shape: torch.Size([32, 93])\n",
      "Batch 162: Input Shape: torch.Size([32, 89]), Label Shape: torch.Size([32, 89])\n",
      "Batch 163: Input Shape: torch.Size([32, 47]), Label Shape: torch.Size([32, 47])\n",
      "Batch 164: Input Shape: torch.Size([32, 34]), Label Shape: torch.Size([32, 34])\n",
      "Batch 165: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 166: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 167: Input Shape: torch.Size([32, 58]), Label Shape: torch.Size([32, 58])\n",
      "Batch 168: Input Shape: torch.Size([32, 69]), Label Shape: torch.Size([32, 69])\n",
      "Batch 169: Input Shape: torch.Size([32, 42]), Label Shape: torch.Size([32, 42])\n",
      "Batch 170: Input Shape: torch.Size([32, 47]), Label Shape: torch.Size([32, 47])\n",
      "Batch 171: Input Shape: torch.Size([32, 75]), Label Shape: torch.Size([32, 75])\n",
      "Batch 172: Input Shape: torch.Size([32, 114]), Label Shape: torch.Size([32, 114])\n",
      "Batch 173: Input Shape: torch.Size([32, 55]), Label Shape: torch.Size([32, 55])\n",
      "Batch 174: Input Shape: torch.Size([32, 95]), Label Shape: torch.Size([32, 95])\n",
      "Batch 175: Input Shape: torch.Size([32, 50]), Label Shape: torch.Size([32, 50])\n",
      "Batch 176: Input Shape: torch.Size([32, 82]), Label Shape: torch.Size([32, 82])\n",
      "Batch 177: Input Shape: torch.Size([32, 80]), Label Shape: torch.Size([32, 80])\n",
      "Batch 178: Input Shape: torch.Size([32, 48]), Label Shape: torch.Size([32, 48])\n",
      "Batch 179: Input Shape: torch.Size([32, 52]), Label Shape: torch.Size([32, 52])\n",
      "Batch 180: Input Shape: torch.Size([32, 70]), Label Shape: torch.Size([32, 70])\n",
      "Batch 181: Input Shape: torch.Size([32, 84]), Label Shape: torch.Size([32, 84])\n",
      "Batch 182: Input Shape: torch.Size([32, 64]), Label Shape: torch.Size([32, 64])\n",
      "Batch 183: Input Shape: torch.Size([32, 81]), Label Shape: torch.Size([32, 81])\n",
      "Batch 184: Input Shape: torch.Size([32, 188]), Label Shape: torch.Size([32, 188])\n",
      "Batch 185: Input Shape: torch.Size([32, 63]), Label Shape: torch.Size([32, 63])\n",
      "Batch 186: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 187: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 188: Input Shape: torch.Size([32, 71]), Label Shape: torch.Size([32, 71])\n",
      "Batch 189: Input Shape: torch.Size([32, 42]), Label Shape: torch.Size([32, 42])\n",
      "Batch 190: Input Shape: torch.Size([32, 63]), Label Shape: torch.Size([32, 63])\n",
      "Batch 191: Input Shape: torch.Size([32, 65]), Label Shape: torch.Size([32, 65])\n",
      "Batch 192: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 193: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 194: Input Shape: torch.Size([32, 85]), Label Shape: torch.Size([32, 85])\n",
      "Batch 195: Input Shape: torch.Size([32, 58]), Label Shape: torch.Size([32, 58])\n",
      "Batch 196: Input Shape: torch.Size([32, 30]), Label Shape: torch.Size([32, 30])\n",
      "Batch 197: Input Shape: torch.Size([32, 69]), Label Shape: torch.Size([32, 69])\n",
      "Batch 198: Input Shape: torch.Size([32, 61]), Label Shape: torch.Size([32, 61])\n",
      "Batch 199: Input Shape: torch.Size([32, 51]), Label Shape: torch.Size([32, 51])\n",
      "Batch 200: Input Shape: torch.Size([32, 123]), Label Shape: torch.Size([32, 123])\n",
      "Batch 201: Input Shape: torch.Size([32, 44]), Label Shape: torch.Size([32, 44])\n",
      "Batch 202: Input Shape: torch.Size([32, 58]), Label Shape: torch.Size([32, 58])\n",
      "Batch 203: Input Shape: torch.Size([32, 75]), Label Shape: torch.Size([32, 75])\n",
      "Batch 204: Input Shape: torch.Size([32, 50]), Label Shape: torch.Size([32, 50])\n",
      "Batch 205: Input Shape: torch.Size([32, 64]), Label Shape: torch.Size([32, 64])\n",
      "Batch 206: Input Shape: torch.Size([32, 51]), Label Shape: torch.Size([32, 51])\n",
      "Batch 207: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 208: Input Shape: torch.Size([32, 74]), Label Shape: torch.Size([32, 74])\n",
      "Batch 209: Input Shape: torch.Size([32, 72]), Label Shape: torch.Size([32, 72])\n",
      "Batch 210: Input Shape: torch.Size([32, 78]), Label Shape: torch.Size([32, 78])\n",
      "Batch 211: Input Shape: torch.Size([32, 83]), Label Shape: torch.Size([32, 83])\n",
      "Batch 212: Input Shape: torch.Size([32, 56]), Label Shape: torch.Size([32, 56])\n",
      "Batch 213: Input Shape: torch.Size([32, 60]), Label Shape: torch.Size([32, 60])\n",
      "Batch 214: Input Shape: torch.Size([32, 58]), Label Shape: torch.Size([32, 58])\n",
      "Batch 215: Input Shape: torch.Size([32, 111]), Label Shape: torch.Size([32, 111])\n",
      "Batch 216: Input Shape: torch.Size([32, 79]), Label Shape: torch.Size([32, 79])\n",
      "Batch 217: Input Shape: torch.Size([32, 54]), Label Shape: torch.Size([32, 54])\n",
      "Batch 218: Input Shape: torch.Size([32, 56]), Label Shape: torch.Size([32, 56])\n",
      "Batch 219: Input Shape: torch.Size([32, 90]), Label Shape: torch.Size([32, 90])\n",
      "Batch 220: Input Shape: torch.Size([32, 82]), Label Shape: torch.Size([32, 82])\n",
      "Batch 221: Input Shape: torch.Size([32, 50]), Label Shape: torch.Size([32, 50])\n",
      "Batch 222: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 223: Input Shape: torch.Size([32, 47]), Label Shape: torch.Size([32, 47])\n",
      "Batch 224: Input Shape: torch.Size([32, 80]), Label Shape: torch.Size([32, 80])\n",
      "Batch 225: Input Shape: torch.Size([32, 46]), Label Shape: torch.Size([32, 46])\n",
      "Batch 226: Input Shape: torch.Size([32, 68]), Label Shape: torch.Size([32, 68])\n",
      "Batch 227: Input Shape: torch.Size([32, 61]), Label Shape: torch.Size([32, 61])\n",
      "Batch 228: Input Shape: torch.Size([32, 58]), Label Shape: torch.Size([32, 58])\n",
      "Batch 229: Input Shape: torch.Size([32, 74]), Label Shape: torch.Size([32, 74])\n",
      "Batch 230: Input Shape: torch.Size([32, 73]), Label Shape: torch.Size([32, 73])\n",
      "Batch 231: Input Shape: torch.Size([32, 70]), Label Shape: torch.Size([32, 70])\n",
      "Batch 232: Input Shape: torch.Size([32, 73]), Label Shape: torch.Size([32, 73])\n",
      "Batch 233: Input Shape: torch.Size([32, 61]), Label Shape: torch.Size([32, 61])\n",
      "Batch 234: Input Shape: torch.Size([32, 52]), Label Shape: torch.Size([32, 52])\n",
      "Batch 235: Input Shape: torch.Size([32, 67]), Label Shape: torch.Size([32, 67])\n",
      "Batch 236: Input Shape: torch.Size([32, 47]), Label Shape: torch.Size([32, 47])\n",
      "Batch 237: Input Shape: torch.Size([32, 64]), Label Shape: torch.Size([32, 64])\n",
      "Batch 238: Input Shape: torch.Size([32, 89]), Label Shape: torch.Size([32, 89])\n",
      "Batch 239: Input Shape: torch.Size([32, 51]), Label Shape: torch.Size([32, 51])\n",
      "Batch 240: Input Shape: torch.Size([32, 48]), Label Shape: torch.Size([32, 48])\n",
      "Batch 241: Input Shape: torch.Size([32, 56]), Label Shape: torch.Size([32, 56])\n",
      "Batch 242: Input Shape: torch.Size([32, 54]), Label Shape: torch.Size([32, 54])\n",
      "Batch 243: Input Shape: torch.Size([32, 77]), Label Shape: torch.Size([32, 77])\n",
      "Batch 244: Input Shape: torch.Size([32, 42]), Label Shape: torch.Size([32, 42])\n",
      "Batch 245: Input Shape: torch.Size([32, 39]), Label Shape: torch.Size([32, 39])\n",
      "Batch 246: Input Shape: torch.Size([32, 41]), Label Shape: torch.Size([32, 41])\n",
      "Batch 247: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 248: Input Shape: torch.Size([32, 69]), Label Shape: torch.Size([32, 69])\n",
      "Batch 249: Input Shape: torch.Size([32, 43]), Label Shape: torch.Size([32, 43])\n",
      "Batch 250: Input Shape: torch.Size([32, 105]), Label Shape: torch.Size([32, 105])\n",
      "Batch 251: Input Shape: torch.Size([32, 45]), Label Shape: torch.Size([32, 45])\n",
      "Batch 252: Input Shape: torch.Size([32, 79]), Label Shape: torch.Size([32, 79])\n",
      "Batch 253: Input Shape: torch.Size([32, 76]), Label Shape: torch.Size([32, 76])\n",
      "Batch 254: Input Shape: torch.Size([32, 129]), Label Shape: torch.Size([32, 129])\n",
      "Batch 255: Input Shape: torch.Size([32, 37]), Label Shape: torch.Size([32, 37])\n",
      "Batch 256: Input Shape: torch.Size([32, 42]), Label Shape: torch.Size([32, 42])\n",
      "Batch 257: Input Shape: torch.Size([32, 67]), Label Shape: torch.Size([32, 67])\n",
      "Batch 258: Input Shape: torch.Size([32, 82]), Label Shape: torch.Size([32, 82])\n",
      "Batch 259: Input Shape: torch.Size([32, 70]), Label Shape: torch.Size([32, 70])\n",
      "Batch 260: Input Shape: torch.Size([32, 56]), Label Shape: torch.Size([32, 56])\n",
      "Batch 261: Input Shape: torch.Size([32, 50]), Label Shape: torch.Size([32, 50])\n",
      "Batch 262: Input Shape: torch.Size([32, 68]), Label Shape: torch.Size([32, 68])\n",
      "Batch 263: Input Shape: torch.Size([32, 64]), Label Shape: torch.Size([32, 64])\n",
      "Batch 264: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 265: Input Shape: torch.Size([32, 44]), Label Shape: torch.Size([32, 44])\n",
      "Batch 266: Input Shape: torch.Size([32, 67]), Label Shape: torch.Size([32, 67])\n",
      "Batch 267: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 268: Input Shape: torch.Size([32, 61]), Label Shape: torch.Size([32, 61])\n",
      "Batch 269: Input Shape: torch.Size([32, 57]), Label Shape: torch.Size([32, 57])\n",
      "Batch 270: Input Shape: torch.Size([32, 32]), Label Shape: torch.Size([32, 32])\n",
      "Batch 271: Input Shape: torch.Size([32, 94]), Label Shape: torch.Size([32, 94])\n",
      "Batch 272: Input Shape: torch.Size([32, 96]), Label Shape: torch.Size([32, 96])\n",
      "Batch 273: Input Shape: torch.Size([32, 72]), Label Shape: torch.Size([32, 72])\n",
      "Batch 274: Input Shape: torch.Size([32, 60]), Label Shape: torch.Size([32, 60])\n",
      "Batch 275: Input Shape: torch.Size([32, 54]), Label Shape: torch.Size([32, 54])\n",
      "Batch 276: Input Shape: torch.Size([32, 65]), Label Shape: torch.Size([32, 65])\n",
      "Batch 277: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 278: Input Shape: torch.Size([32, 69]), Label Shape: torch.Size([32, 69])\n",
      "Batch 279: Input Shape: torch.Size([32, 45]), Label Shape: torch.Size([32, 45])\n",
      "Batch 280: Input Shape: torch.Size([32, 59]), Label Shape: torch.Size([32, 59])\n",
      "Batch 281: Input Shape: torch.Size([32, 89]), Label Shape: torch.Size([32, 89])\n",
      "Batch 282: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 283: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 284: Input Shape: torch.Size([32, 69]), Label Shape: torch.Size([32, 69])\n",
      "Batch 285: Input Shape: torch.Size([32, 40]), Label Shape: torch.Size([32, 40])\n",
      "Batch 286: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 287: Input Shape: torch.Size([32, 105]), Label Shape: torch.Size([32, 105])\n",
      "Batch 288: Input Shape: torch.Size([32, 68]), Label Shape: torch.Size([32, 68])\n",
      "Batch 289: Input Shape: torch.Size([32, 42]), Label Shape: torch.Size([32, 42])\n",
      "Batch 290: Input Shape: torch.Size([32, 63]), Label Shape: torch.Size([32, 63])\n",
      "Batch 291: Input Shape: torch.Size([32, 63]), Label Shape: torch.Size([32, 63])\n",
      "Batch 292: Input Shape: torch.Size([32, 42]), Label Shape: torch.Size([32, 42])\n",
      "Batch 293: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 294: Input Shape: torch.Size([32, 68]), Label Shape: torch.Size([32, 68])\n",
      "Batch 295: Input Shape: torch.Size([32, 40]), Label Shape: torch.Size([32, 40])\n",
      "Batch 296: Input Shape: torch.Size([32, 55]), Label Shape: torch.Size([32, 55])\n",
      "Batch 297: Input Shape: torch.Size([32, 106]), Label Shape: torch.Size([32, 106])\n",
      "Batch 298: Input Shape: torch.Size([32, 81]), Label Shape: torch.Size([32, 81])\n",
      "Batch 299: Input Shape: torch.Size([32, 65]), Label Shape: torch.Size([32, 65])\n",
      "Batch 300: Input Shape: torch.Size([32, 74]), Label Shape: torch.Size([32, 74])\n",
      "Batch 301: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 302: Input Shape: torch.Size([32, 52]), Label Shape: torch.Size([32, 52])\n",
      "Batch 303: Input Shape: torch.Size([32, 77]), Label Shape: torch.Size([32, 77])\n",
      "Batch 304: Input Shape: torch.Size([32, 56]), Label Shape: torch.Size([32, 56])\n",
      "Batch 305: Input Shape: torch.Size([32, 66]), Label Shape: torch.Size([32, 66])\n",
      "Batch 306: Input Shape: torch.Size([32, 102]), Label Shape: torch.Size([32, 102])\n",
      "Batch 307: Input Shape: torch.Size([32, 73]), Label Shape: torch.Size([32, 73])\n",
      "Batch 308: Input Shape: torch.Size([32, 51]), Label Shape: torch.Size([32, 51])\n",
      "Batch 309: Input Shape: torch.Size([32, 70]), Label Shape: torch.Size([32, 70])\n",
      "Batch 310: Input Shape: torch.Size([32, 72]), Label Shape: torch.Size([32, 72])\n",
      "Batch 311: Input Shape: torch.Size([32, 81]), Label Shape: torch.Size([32, 81])\n",
      "Batch 312: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 313: Input Shape: torch.Size([32, 50]), Label Shape: torch.Size([32, 50])\n",
      "Batch 314: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 315: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 316: Input Shape: torch.Size([32, 40]), Label Shape: torch.Size([32, 40])\n",
      "Batch 317: Input Shape: torch.Size([32, 86]), Label Shape: torch.Size([32, 86])\n",
      "Batch 318: Input Shape: torch.Size([32, 63]), Label Shape: torch.Size([32, 63])\n",
      "Batch 319: Input Shape: torch.Size([32, 47]), Label Shape: torch.Size([32, 47])\n",
      "Batch 320: Input Shape: torch.Size([32, 61]), Label Shape: torch.Size([32, 61])\n",
      "Batch 321: Input Shape: torch.Size([32, 46]), Label Shape: torch.Size([32, 46])\n",
      "Batch 322: Input Shape: torch.Size([32, 60]), Label Shape: torch.Size([32, 60])\n",
      "Batch 323: Input Shape: torch.Size([32, 48]), Label Shape: torch.Size([32, 48])\n",
      "Batch 324: Input Shape: torch.Size([32, 51]), Label Shape: torch.Size([32, 51])\n",
      "Batch 325: Input Shape: torch.Size([32, 46]), Label Shape: torch.Size([32, 46])\n",
      "Batch 326: Input Shape: torch.Size([32, 50]), Label Shape: torch.Size([32, 50])\n",
      "Batch 327: Input Shape: torch.Size([32, 57]), Label Shape: torch.Size([32, 57])\n",
      "Batch 328: Input Shape: torch.Size([32, 41]), Label Shape: torch.Size([32, 41])\n",
      "Batch 329: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 330: Input Shape: torch.Size([32, 77]), Label Shape: torch.Size([32, 77])\n",
      "Batch 331: Input Shape: torch.Size([32, 48]), Label Shape: torch.Size([32, 48])\n",
      "Batch 332: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 333: Input Shape: torch.Size([32, 48]), Label Shape: torch.Size([32, 48])\n",
      "Batch 334: Input Shape: torch.Size([32, 51]), Label Shape: torch.Size([32, 51])\n",
      "Batch 335: Input Shape: torch.Size([32, 65]), Label Shape: torch.Size([32, 65])\n",
      "Batch 336: Input Shape: torch.Size([32, 51]), Label Shape: torch.Size([32, 51])\n",
      "Batch 337: Input Shape: torch.Size([32, 57]), Label Shape: torch.Size([32, 57])\n",
      "Batch 338: Input Shape: torch.Size([32, 42]), Label Shape: torch.Size([32, 42])\n",
      "Batch 339: Input Shape: torch.Size([32, 69]), Label Shape: torch.Size([32, 69])\n",
      "Batch 340: Input Shape: torch.Size([32, 68]), Label Shape: torch.Size([32, 68])\n",
      "Batch 341: Input Shape: torch.Size([32, 81]), Label Shape: torch.Size([32, 81])\n",
      "Batch 342: Input Shape: torch.Size([32, 168]), Label Shape: torch.Size([32, 168])\n",
      "Batch 343: Input Shape: torch.Size([32, 66]), Label Shape: torch.Size([32, 66])\n",
      "Batch 344: Input Shape: torch.Size([32, 76]), Label Shape: torch.Size([32, 76])\n",
      "Batch 345: Input Shape: torch.Size([32, 43]), Label Shape: torch.Size([32, 43])\n",
      "Batch 346: Input Shape: torch.Size([32, 88]), Label Shape: torch.Size([32, 88])\n",
      "Batch 347: Input Shape: torch.Size([32, 60]), Label Shape: torch.Size([32, 60])\n",
      "Batch 348: Input Shape: torch.Size([32, 56]), Label Shape: torch.Size([32, 56])\n",
      "Batch 349: Input Shape: torch.Size([32, 71]), Label Shape: torch.Size([32, 71])\n",
      "Batch 350: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 351: Input Shape: torch.Size([32, 59]), Label Shape: torch.Size([32, 59])\n",
      "Batch 352: Input Shape: torch.Size([32, 59]), Label Shape: torch.Size([32, 59])\n",
      "Batch 353: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 354: Input Shape: torch.Size([32, 66]), Label Shape: torch.Size([32, 66])\n",
      "Batch 355: Input Shape: torch.Size([32, 128]), Label Shape: torch.Size([32, 128])\n",
      "Batch 356: Input Shape: torch.Size([32, 43]), Label Shape: torch.Size([32, 43])\n",
      "Batch 357: Input Shape: torch.Size([32, 39]), Label Shape: torch.Size([32, 39])\n",
      "Batch 358: Input Shape: torch.Size([32, 78]), Label Shape: torch.Size([32, 78])\n",
      "Batch 359: Input Shape: torch.Size([32, 77]), Label Shape: torch.Size([32, 77])\n",
      "Batch 360: Input Shape: torch.Size([32, 57]), Label Shape: torch.Size([32, 57])\n",
      "Batch 361: Input Shape: torch.Size([32, 60]), Label Shape: torch.Size([32, 60])\n",
      "Batch 362: Input Shape: torch.Size([32, 45]), Label Shape: torch.Size([32, 45])\n",
      "Batch 363: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 364: Input Shape: torch.Size([32, 53]), Label Shape: torch.Size([32, 53])\n",
      "Batch 365: Input Shape: torch.Size([32, 185]), Label Shape: torch.Size([32, 185])\n",
      "Batch 366: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 367: Input Shape: torch.Size([32, 62]), Label Shape: torch.Size([32, 62])\n",
      "Batch 368: Input Shape: torch.Size([32, 61]), Label Shape: torch.Size([32, 61])\n",
      "Batch 369: Input Shape: torch.Size([32, 73]), Label Shape: torch.Size([32, 73])\n",
      "Batch 370: Input Shape: torch.Size([32, 60]), Label Shape: torch.Size([32, 60])\n",
      "Batch 371: Input Shape: torch.Size([32, 45]), Label Shape: torch.Size([32, 45])\n",
      "Batch 372: Input Shape: torch.Size([32, 65]), Label Shape: torch.Size([32, 65])\n",
      "Batch 373: Input Shape: torch.Size([32, 70]), Label Shape: torch.Size([32, 70])\n",
      "Batch 374: Input Shape: torch.Size([32, 122]), Label Shape: torch.Size([32, 122])\n",
      "Batch 375: Input Shape: torch.Size([32, 43]), Label Shape: torch.Size([32, 43])\n",
      "Batch 376: Input Shape: torch.Size([32, 77]), Label Shape: torch.Size([32, 77])\n",
      "Batch 377: Input Shape: torch.Size([32, 46]), Label Shape: torch.Size([32, 46])\n",
      "Batch 378: Input Shape: torch.Size([32, 55]), Label Shape: torch.Size([32, 55])\n",
      "Batch 379: Input Shape: torch.Size([32, 80]), Label Shape: torch.Size([32, 80])\n",
      "Batch 380: Input Shape: torch.Size([32, 58]), Label Shape: torch.Size([32, 58])\n",
      "Batch 381: Input Shape: torch.Size([32, 56]), Label Shape: torch.Size([32, 56])\n",
      "Batch 382: Input Shape: torch.Size([32, 63]), Label Shape: torch.Size([32, 63])\n",
      "Batch 383: Input Shape: torch.Size([32, 45]), Label Shape: torch.Size([32, 45])\n",
      "Batch 384: Input Shape: torch.Size([32, 65]), Label Shape: torch.Size([32, 65])\n",
      "Batch 385: Input Shape: torch.Size([32, 63]), Label Shape: torch.Size([32, 63])\n",
      "Batch 386: Input Shape: torch.Size([32, 95]), Label Shape: torch.Size([32, 95])\n",
      "Batch 387: Input Shape: torch.Size([32, 54]), Label Shape: torch.Size([32, 54])\n",
      "Batch 388: Input Shape: torch.Size([32, 49]), Label Shape: torch.Size([32, 49])\n",
      "Batch 389: Input Shape: torch.Size([32, 73]), Label Shape: torch.Size([32, 73])\n",
      "Batch 390: Input Shape: torch.Size([32, 55]), Label Shape: torch.Size([32, 55])\n",
      "Batch 391: Input Shape: torch.Size([32, 59]), Label Shape: torch.Size([32, 59])\n",
      "Batch 392: Input Shape: torch.Size([31, 46]), Label Shape: torch.Size([31, 46])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "label_map = {'B-PER': 0, 'I-PER': 1, 'O': 2, 'B-LOC':3, 'I-LOC':4, 'B-ORG':5, 'I-ORG': 6}\n",
    "\n",
    "def my_collate_fn(batch):\n",
    "    inputs = [torch.tensor([token_to_index.get(token, 0) for token in item[0]]) for item in batch]\n",
    "    labels = [torch.tensor([label_map[label] for label in item[1]]) for item in batch]\n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True, padding_value=0)\n",
    "    padded_labels = pad_sequence(labels, batch_first=True, padding_value=-1) \n",
    "    return padded_inputs, padded_labels\n",
    "\n",
    "token_to_index = {}\n",
    "\n",
    "train_dataloader = DataLoader(train_data_formated, batch_size=32, shuffle=True, collate_fn=my_collate_fn)\n",
    "\n",
    "for batch_idx, (inputs, labels) in enumerate(train_dataloader):\n",
    "    print(f\"Batch {batch_idx + 1}: Input Shape: {inputs.shape}, Label Shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping the labels to indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 51\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# reset the gradient\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m#model.zero_grad()\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# loop over batches\u001b[39;00m\n\u001b[0;32m     49\u001b[0m counter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mtrain_batches\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]): \u001b[38;5;66;03m#TODO\u001b[39;00m\n\u001b[0;32m     52\u001b[0m     predicted_values \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mforward(train_batches[batch])\n\u001b[0;32m     53\u001b[0m     flattened_output \u001b[38;5;241m=\u001b[39m predicted_values\u001b[38;5;241m.\u001b[39mview(BATCH_SIZE \u001b[38;5;241m*\u001b[39m max_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_batches' is not defined"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5\n",
    "n_labels = 7 \n",
    "\n",
    "class NER_Tagger(torch.nn.Module):\n",
    "    def __init__(self, n_labels):\n",
    "        super().__init__()\n",
    "        # TODO\n",
    "        # bert includes a linear layer \n",
    "        #self.bert = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=n_labels)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased', num_labels=n_labels)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.linear = nn.Linear(self.hidden_size, n_labels)\n",
    "        self.softmax = nn.Softmax(dim = 2)\n",
    "        \n",
    "    def forward(self, inputData):\n",
    "        # TODO\n",
    "        # bert model output \n",
    "        output_bert = self.bert(inputData)\n",
    "        #logits = output_bert.logits\n",
    "        logits = self.linear(output_bert)\n",
    "        # get probabilities \n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "\n",
    "    def predict(self, inputData): \n",
    "        prediction_output = self.forward(inputData) \n",
    "        prediction = torch.argmax(prediction_output, dim = 2)\n",
    "        return prediction \n",
    "\n",
    "\n",
    "\n",
    "model = NER_Tagger(n_labels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_function = torch.nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # set model in training mode \n",
    "    model.train()\n",
    "    # reset the gradient\n",
    "    #model.zero_grad()\n",
    "    # loop over batches\n",
    "    counter = 0\n",
    "\n",
    "    for batch in range(train_batches.shape[0]): #TODO\n",
    "        predicted_values = model.forward(train_batches[batch])\n",
    "        flattened_output = predicted_values.view(BATCH_SIZE * max_len, -1)\n",
    "        flattened_labels = train_label_batches[batch].view(-1).long()\n",
    "        # calculate loss (and print)\n",
    "        # print(flattened_output.shape)\n",
    "        # print(sample_label_batches[batch].shape)\n",
    "        # print(flattened_labels)\n",
    "        # print(flattened_labels.long())\n",
    "        loss = loss_function(flattened_output, flattened_labels)\n",
    "        #print(loss.item())\n",
    "        # print some of the losses: \n",
    "        if counter % 100==0:\n",
    "            print(f\"loss: {loss:>7f}\")\n",
    "        # update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        counter +=1\n",
    "        # TODO\n",
    "        \n",
    "# set to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2645: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (512) to match target batch_size (5).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[0;32m     46\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: batch[\u001b[38;5;241m2\u001b[39m]}\n\u001b[1;32m---> 47\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m     loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     49\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1778\u001b[0m, in \u001b[0;36mBertForTokenClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1777\u001b[0m     loss_fct \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[1;32m-> 1778\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1781\u001b[0m     output \u001b[38;5;241m=\u001b[39m (logits,) \u001b[38;5;241m+\u001b[39m outputs[\u001b[38;5;241m2\u001b[39m:]\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1180\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1181\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected input batch_size (512) to match target batch_size (5)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "# Example tokenized data\n",
    "tokenized_data = [(['John', 'Doe', 'lives', 'in', 'Paris'], ['B-PER', 'I-PER', 'O', 'O', 'B-LOC'])]\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Convert tokenized data to features\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "labels = []\n",
    "for tokens, bio_tags in tokenized_data:\n",
    "    # Convert tokens to token IDs\n",
    "    encoded_dict = tokenizer.encode_plus(tokens, add_special_tokens=True, max_length=512, pad_to_max_length=True, return_attention_mask=True, return_tensors='pt')\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert BIO tags to label IDs\n",
    "    label_map = {'B-PER': 0, 'I-PER': 1, 'O': 2, 'B-LOC':3, 'I-LOC':4, 'B-ORG':5, 'I-ORG': 6}  # Define your label mapping\n",
    "    label_ids = [label_map[tag] for tag in bio_tags]\n",
    "    labels.append(label_ids)\n",
    "\n",
    "# Convert lists to tensors\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Define the model\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=3)  # 3 labels: B, I, O\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "dataloader = DataLoader(dataset, sampler=RandomSampler(dataset), batch_size=32)\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for _ in range(epochs):\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "# Evaluation (optional)\n",
    "model.eval()\n",
    "# Evaluation code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict\n",
    "import codecs\n",
    "import torch\n",
    "import sys\n",
    "import myutils\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# set seed for consistency\n",
    "torch.manual_seed(8446)\n",
    "# Set some constants\n",
    "BERT = 'bert-base-multilingual-cased'\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.00001\n",
    "EPOCHS = 3\n",
    "# We have an UNK label for robustness purposes, it makes it easier to run on\n",
    "# data with other labels, or without labels.\n",
    "UNK = \"[UNK]\"\n",
    "MAX_TRAIN_SENTS=64\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "class ClassModel(torch.nn.Module):\n",
    "    def __init__(self, nlabels: int, bert: str):\n",
    "        \"\"\"\n",
    "        Model for classification with transformers.\n",
    "\n",
    "        The architecture of this model is simple, we just have a transformer\n",
    "        based language model, and add one linear layer to converts it output\n",
    "        to our prediction.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        nlabels : int\n",
    "            Vocabulary size of output space (i.e. number of labels)\n",
    "        bert : str\n",
    "            Name of the transformers language model to use, can be found on:\n",
    "            https://huggingface.co/models\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # The transformer model to use\n",
    "        self.bert = AutoModel.from_pretrained(bert)\n",
    "\n",
    "        # Find the size of the output of the masked language model\n",
    "        if hasattr(self.bert.config, 'hidden_size'):\n",
    "            self.bert_out_size = self.bert.config.hidden_size\n",
    "        elif hasattr(self.bert.config, 'dim'):\n",
    "            self.bert_out_size = self.bert.config.dim\n",
    "        else: # if not found, guess\n",
    "            self.bert_out_size = 768\n",
    "\n",
    "        # Create prediction layer\n",
    "        self.hidden_to_label = torch.nn.Linear(self.bert_out_size, nlabels)\n",
    "\n",
    "    def forward(self, input: torch.tensor):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        input : torch.tensor\n",
    "            Tensor with wordpiece indices. shape=(batch_size, max_sent_len).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        output_scores : torch.tensor\n",
    "            ?. shape=(?,?)\n",
    "        \"\"\"\n",
    "        # Run transformer model on input\n",
    "        bert_out = self.bert(input)\n",
    "\n",
    "        # Keep only the last layer: shape=(batch_size, max_len, DIM_EMBEDDING)\n",
    "        bert_out = bert_out.last_hidden_state\n",
    "        # Keep only the output for the first ([CLS]) token: shape=(batch_size, DIM_EMBEDDING)\n",
    "        bert_out = bert_out[:,:1,:].squeeze()\n",
    "\n",
    "        # Matrix multiply to get scores for each label: shape=(?,?)\n",
    "        output_scores = self.hidden_to_label(bert_out)\n",
    "\n",
    "        return output_scores\n",
    "\n",
    "    def run_eval(self, text_batched: List[torch.tensor], labels_batched: List[torch.tensor]):\n",
    "        \"\"\"\n",
    "        Run evaluation: predict and score\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        text_batched : List[torch.tensor]\n",
    "            list with batches of text, containing wordpiece indices.\n",
    "        labels_batched : List[torch.tensor]\n",
    "            list with batches of labels (converted to ints).\n",
    "        model : torch.nn.module\n",
    "            The model to use for prediction.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            accuracy of model on labels_batches given feats_batches\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        match = 0\n",
    "        total = 0\n",
    "        for sents, labels in zip(text_batched, labels_batched):\n",
    "            output_scores = self.forward(sents)\n",
    "            pred_labels = torch.argmax(output_scores, 1)\n",
    "            for gold_label, pred_label in zip(labels, pred_labels):\n",
    "                total += 1\n",
    "                if gold_label.item() == pred_label.item():\n",
    "                    match+= 1\n",
    "        return(match/total)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
