{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.30.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\obe\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.1.2)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (2023.12.2)\n",
      "Requirement already satisfied: requests in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\obe\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: transformers in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\obe\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.11.17)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.18.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.26.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2023.12.2)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\obe\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seqeval) (1.26.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\obe\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate -U\n",
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, Trainer\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "! IMPORTANT \n",
    "To run the testing you need to have a trained model that is already saved as well as a pickle of the label_indices from training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify the path of the testing set and the path for saving the file with predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The only thing you have to change is the language of the model you're working with\n",
    "model_language = \"serbian\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# List of test data file paths\n",
    "test_data_files = [r\"data\\zh_pud-ud-test.iob2\",\n",
    "                   r\"data\\da_ddt-ud-test.iob2\",\n",
    "                   r\"data\\en_ewt-ud-test.iob2\",\n",
    "                   r\"data\\de_pud-ud-test.iob2\",\n",
    "                   r\"data\\pt_pud-ud-test.iob2\",\n",
    "                   r\"data\\ru_pud-ud-test.iob2\",\n",
    "                   r\"data\\sk_snk-ud-test.iob2\",\n",
    "                   r\"data\\sv_pud-ud-test.iob2\",\n",
    "                   r\"data\\sr_test.iob2\",\n",
    "                   r\"data\\hr_test.iob2\",\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "# where should we pull the model from\n",
    "hub_folder = f\"annamariagnat/NEW_trained_{model_language}\"\n",
    "\n",
    "# specify a path to your label_indices\n",
    "labels = f\"01_label_indices/NEW_labels_{model_language}.pkl\"\n",
    "\n",
    "label_all_tokens = True # dw about it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeated the functions needed to load in and format the test dataset\n",
    "\n",
    "#reading in data as a dataframe\n",
    "def read_iob2_file(path):\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[1])\n",
    "            current_tags.append(tok[2])\n",
    "        else:\n",
    "            if current_words:\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['words', 'tags'])\n",
    "    df['id'] = df.index\n",
    "    df = df[['id', 'words', 'tags']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# tokenizing the labels\n",
    "def tokenize_and_align_labels(dataset, word_column, tag_column, tokenizer):\n",
    "    tokenized_inputs = tokenizer(dataset[word_column].tolist(), truncation=True , is_split_into_words=True, padding = True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(dataset[tag_column]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs.data\n",
    "\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, pad_unk='<PAD>'):\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word is None or word == self.pad_unk:\n",
    "            return None\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                idx = len(self.idx2word)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word.append(word)\n",
    "                return idx\n",
    "            else:\n",
    "                return None\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99d334b7232e4349952717df171eb1ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Password for the hub (Anna)\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d76e363d2e9a40d4bbb115a9c6f83922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/958 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\obe\\.cache\\huggingface\\hub\\models--annamariagnat--NEW_trained_serbian. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71f8e54fe6354aa8be014954c1527fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/539M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97b69cde624d4e6996934bf97dd69af9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4438ec22539a40d3a831be2c0599bcf1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20321e3a00794b379cdd6d6aaf9cddf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.92M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473cb14f3e5c48d08d5fdeab4cc63f0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loaded_model = AutoModelForTokenClassification.from_pretrained(hub_folder)\n",
    "trainer = Trainer(model = loaded_model)\n",
    "tokenizer = tokenizer = AutoTokenizer.from_pretrained(hub_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load label_indices dictionary from the pickle file\n",
    "with open(labels, 'rb') as f:\n",
    "    label_indices = pickle.load(f)\n",
    "\n",
    "label_list = label_indices.idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\obe\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data\\zh_pud-ud-test.iob2:\n",
      "{'LOC': {'precision': 0.6352288488210819, 'recall': 0.6325966850828729, 'f1': 0.6339100346020761, 'number': 1448}, 'ORG': {'precision': 0.35610766045548653, 'recall': 0.45989304812834225, 'f1': 0.40140023337222874, 'number': 374}, 'PER': {'precision': 0.7288267288267288, 'recall': 0.7797173732335827, 'f1': 0.753413654618474, 'number': 1203}, 'overall_precision': 0.6307596513075965, 'overall_recall': 0.6697520661157025, 'overall_f1': 0.6496713163379829, 'overall_accuracy': 0.945479395073833}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f03cee3695147959ba24bd52822c13b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data\\da_ddt-ud-test.iob2:\n",
      "{'LOC': {'precision': 0.591743119266055, 'recall': 0.6789473684210526, 'f1': 0.6323529411764707, 'number': 190}, 'ORG': {'precision': 0.5531197301854974, 'recall': 0.6393762183235867, 'f1': 0.5931283905967449, 'number': 513}, 'PER': {'precision': 0.7639751552795031, 'recall': 0.7299703264094956, 'f1': 0.7465857359635812, 'number': 337}, 'overall_precision': 0.6204766107678729, 'overall_recall': 0.6759615384615385, 'overall_f1': 0.6470317533364013, 'overall_accuracy': 0.9581347417235605}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8878f02f25694aee964117b9f2617304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data\\en_ewt-ud-test.iob2:\n",
      "{'LOC': {'precision': 0.6666666666666666, 'recall': 0.4979757085020243, 'f1': 0.5701042873696407, 'number': 494}, 'ORG': {'precision': 0.12871287128712872, 'recall': 0.4406779661016949, 'f1': 0.19923371647509577, 'number': 531}, 'PER': {'precision': 0.6961602671118531, 'recall': 0.600864553314121, 'f1': 0.6450116009280742, 'number': 694}, 'overall_precision': 0.32196697774587224, 'overall_recall': 0.5218150087260035, 'overall_f1': 0.3982241953385128, 'overall_accuracy': 0.919816933638444}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbbad472a324b39ba39882260a8840a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data\\de_pud-ud-test.iob2:\n",
      "{'LOC': {'precision': 0.6562847608453838, 'recall': 0.8149171270718232, 'f1': 0.7270486752926679, 'number': 724}, 'ORG': {'precision': 0.36086956521739133, 'recall': 0.5684931506849316, 'f1': 0.4414893617021277, 'number': 292}, 'PER': {'precision': 0.8050974512743628, 'recall': 0.7816593886462883, 'f1': 0.7932053175775481, 'number': 687}, 'overall_precision': 0.638203356367226, 'overall_recall': 0.7592483852025836, 'overall_f1': 0.6934835076427996, 'overall_accuracy': 0.965569774527727}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373447bd23db456ebbd96333fa540101",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data\\pt_pud-ud-test.iob2:\n",
      "{'LOC': {'precision': 0.6988922457200403, 'recall': 0.8060394889663183, 'f1': 0.7486515641855449, 'number': 861}, 'ORG': {'precision': 0.42549019607843136, 'recall': 0.7431506849315068, 'f1': 0.541147132169576, 'number': 292}, 'PER': {'precision': 0.7811594202898551, 'recall': 0.8044776119402985, 'f1': 0.7926470588235295, 'number': 670}, 'overall_precision': 0.6611947104423165, 'overall_recall': 0.7953922106417992, 'overall_f1': 0.7221115537848605, 'overall_accuracy': 0.9681145113524185}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a6c679cd034df0a4ce509cd14c80e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data\\ru_pud-ud-test.iob2:\n",
      "{'LOC': {'precision': 0.5624515128006207, 'recall': 0.7382892057026477, 'f1': 0.6384852487890798, 'number': 982}, 'ORG': {'precision': 0.42244224422442245, 'recall': 0.3764705882352941, 'f1': 0.39813374805598756, 'number': 680}, 'PER': {'precision': 0.8285714285714286, 'recall': 0.8537020517395183, 'f1': 0.8409490333919156, 'number': 1121}, 'overall_precision': 0.6354098360655738, 'overall_recall': 0.6963708228530363, 'overall_f1': 0.6644951140065147, 'overall_accuracy': 0.951548377537413}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fef7a39b3494a6ca71604e6b1aa826e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data\\sk_snk-ud-test.iob2:\n",
      "{'LOC': {'precision': 0.7318611987381703, 'recall': 0.7733333333333333, 'f1': 0.7520259319286872, 'number': 900}, 'ORG': {'precision': 0.2948207171314741, 'recall': 0.5401459854014599, 'f1': 0.38144329896907225, 'number': 137}, 'PER': {'precision': 0.7652982184353214, 'recall': 0.7128427128427128, 'f1': 0.7381397086290624, 'number': 1386}, 'overall_precision': 0.7051744885679904, 'overall_recall': 0.725546842756913, 'overall_f1': 0.7152156224572824, 'overall_accuracy': 0.9532010042366232}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce2406eb1a84af1920b356af73d4e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data\\sv_pud-ud-test.iob2:\n",
      "{'LOC': {'precision': 0.7827191867852605, 'recall': 0.7503045066991474, 'f1': 0.7661691542288558, 'number': 821}, 'ORG': {'precision': 0.4411764705882353, 'recall': 0.5603448275862069, 'f1': 0.49367088607594933, 'number': 348}, 'PER': {'precision': 0.8188736681887366, 'recall': 0.7752161383285303, 'f1': 0.7964470762398224, 'number': 694}, 'overall_precision': 0.7152704135737009, 'overall_recall': 0.7241009125067096, 'overall_f1': 0.7196585756201652, 'overall_accuracy': 0.9724307490575316}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0d7cb94059840a581c66aaee7c3c817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data\\sr_test.iob2:\n",
      "{'LOC': {'precision': 0.9432624113475178, 'recall': 0.9736456808199122, 'f1': 0.9582132564841499, 'number': 683}, 'ORG': {'precision': 0.8469387755102041, 'recall': 0.8615916955017301, 'f1': 0.8542024013722127, 'number': 578}, 'PER': {'precision': 0.9211538461538461, 'recall': 0.9410609037328095, 'f1': 0.9310009718172982, 'number': 509}, 'overall_precision': 0.9056811913954771, 'overall_recall': 0.927683615819209, 'overall_f1': 0.9165503767792352, 'overall_accuracy': 0.9842645154865053}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6bd7f08a3cc4e3a9cf59eaec70efe5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for data\\hr_test.iob2:\n",
      "{'LOC': {'precision': 0.902135231316726, 'recall': 0.9094170403587444, 'f1': 0.9057615006699419, 'number': 1115}, 'ORG': {'precision': 0.7407087294727744, 'recall': 0.825626204238921, 'f1': 0.780865603644647, 'number': 1038}, 'PER': {'precision': 0.886150234741784, 'recall': 0.9009546539379475, 'f1': 0.893491124260355, 'number': 838}, 'overall_precision': 0.8381742738589212, 'overall_recall': 0.8779672350384486, 'overall_f1': 0.8576094056172435, 'overall_accuracy': 0.9797745511898687}\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")\n",
    "for test_path in test_data_files:\n",
    "    test_data = read_iob2_file(test_path)\n",
    "\n",
    "    test_data['tag_idx'] = test_data['tags'].apply(lambda x: [label_indices.word2idx[tag] for tag in x])\n",
    "\n",
    "    # tokenize the data\n",
    "    tokenized_test_data = tokenize_and_align_labels(test_data, \"words\", \"tag_idx\", tokenizer)\n",
    "\n",
    "\n",
    "    # turning the data into datasetdicts, to make them compatible with the trainer (otherwise they can't be indexed)\n",
    "\n",
    "    test_dataset = Dataset.from_dict({\n",
    "        'id': range(len(tokenized_test_data['input_ids'])),\n",
    "        'input_ids': tokenized_test_data['input_ids'],\n",
    "        'attention_mask': tokenized_test_data['attention_mask'],\n",
    "        'labels': tokenized_test_data['labels']\n",
    "    })\n",
    "\n",
    "    # evaluating using test data\n",
    "    test_dataset_new = Dataset.from_dict({\n",
    "        'input_ids': test_dataset['input_ids'],\n",
    "        'attention_mask': test_dataset['attention_mask'],\n",
    "        'labels': test_dataset['labels']\n",
    "    })\n",
    "\n",
    "    predictions, labels, _ = trainer.predict(test_dataset_new)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    print(f\"Results for {test_path}:\")\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions needed for the testing phase\n",
    "\n",
    "def un_tok_labs(list_of_labels, list_of_words):\n",
    "    tokenized_inputs = tokenizer(list_of_words, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(list_of_labels):\n",
    "        \n",
    "        label_copy = label.copy()  # Create a copy of the label list\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:  # Only label the first token of a given word.\n",
    "                continue\n",
    "            elif word_idx == previous_word_idx:\n",
    "                label_copy.pop(word_idx)\n",
    "                continue\n",
    "            else:\n",
    "                label_ids.append(label_copy[word_idx])\n",
    "            previous_word_idx = word_idx \n",
    "        labels.append(label_ids)\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "def read_list_of_words(path):\n",
    "    \"\"\"\n",
    "    read in iob2 file\n",
    "    \n",
    "    :param path: path to read from\n",
    "    :returns: list with sequences of words for each sentence\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    current_words = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue # skip comments\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[1])\n",
    "        else:\n",
    "            if current_words:  # skip empty lines\n",
    "                data.append(current_words)\n",
    "            current_words = []\n",
    "\n",
    "    # Check for the last sentence\n",
    "    if current_words:\n",
    "        data.append(current_words)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "def save_preds(filename, tok, untok_labs):\n",
    "    file_path = \"data/\" + filename\n",
    "    \n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"The file {filename} already exists. Skipping saving it.\")\n",
    "        return  # Skip saving the file if it already exists\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f: \n",
    "        for t, l in zip(tok, untok_labs): \n",
    "            for i in range(len(t)): \n",
    "                f.write(f\"{i+1}\\t{t[i]}\\t{l[i]}\\n\")\n",
    "\n",
    "            f.write(\"\\n\")\n",
    "    \n",
    "    return \"File has been saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccafb0af139434fa3d3810ed039f6e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cf65483902a428983f7d83d98383cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e41ae088d01e4e498b0b7f4adb1904f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/260 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdeeb13e8aa4d1890f05f6207462b59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4fbd50f4cd42e8a7c3c717be283b03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a27776aa69843b5a989f928619d5611",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e12c8c053ce4c6d80ad3a7cdb22362b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a34590e3296c43ef8326d2157af3f24f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a01116a55c04d78bb090bef5ea1a054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/65 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e63a6387c846e1b990886c96790aa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/142 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for test_path in test_data_files:\n",
    "    testing_language_code = os.path.basename(test_path).split('_')[0]  # Extracting the language code from the file name\n",
    "    output_filename = f\"NEW_{model_language}_on_{testing_language_code}_predictions\"\n",
    "\n",
    "    test_data = read_iob2_file(test_path)\n",
    "    test_data['tag_idx'] = test_data['tags'].apply(lambda x: [label_indices.word2idx[tag] for tag in x])\n",
    "    tokenized_test_data = tokenize_and_align_labels(test_data, \"words\", \"tag_idx\", tokenizer)\n",
    "\n",
    "    test_dataset = Dataset.from_dict({\n",
    "        'id': range(len(tokenized_test_data['input_ids'])),\n",
    "        'input_ids': tokenized_test_data['input_ids'],\n",
    "        'attention_mask': tokenized_test_data['attention_mask'],\n",
    "        'labels': tokenized_test_data['labels']\n",
    "    })\n",
    "\n",
    "    test_dataset_new = Dataset.from_dict({\n",
    "        'input_ids': test_dataset['input_ids'],\n",
    "        'attention_mask': test_dataset['attention_mask'],\n",
    "        'labels': test_dataset['labels']\n",
    "    })\n",
    "\n",
    "    # Perform testing phase\n",
    "    predictions, labels, _ = trainer.predict(test_dataset_new)\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    test_words = read_list_of_words(test_path)\n",
    "    untok_labs = un_tok_labs(true_predictions, test_words)\n",
    "\n",
    "    # Save predictions\n",
    "    save_preds(output_filename, test_words, untok_labs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
