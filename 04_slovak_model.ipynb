{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Checking all installs, importing libraries and loading all the functions we'll use\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (23.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\annam\\appdata\\roaming\\python\\python311\\site-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (2.1.2+cu118)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from accelerate) (0.4.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (4.4.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.10.0->accelerate) (2024.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub->accelerate) (4.66.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\annam\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.42.1->huggingface-hub->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->huggingface-hub->accelerate) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.38.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\annam\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.18.0)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: filelock in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.9.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (15.0.2)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.2.0,>=2023.1.0 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2024.2.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.21.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub>=0.19.4->datasets) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\annam\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seqeval in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seqeval) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\annam\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install accelerate -U\n",
    "%pip install transformers\n",
    "%pip install datasets\n",
    "%pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from datasets import load_metric\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from huggingface_hub import notebook_login\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the definitions can be found in the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions of all functions\n",
    "\n",
    "\n",
    "#reading in data as a dataframe\n",
    "def read_iob2_file(path):\n",
    "    data = []\n",
    "    current_words = []\n",
    "    current_tags = []\n",
    "\n",
    "    for line in open(path, encoding='utf-8'):\n",
    "        line = line.strip()\n",
    "\n",
    "        if line:\n",
    "            if line[0] == '#':\n",
    "                continue\n",
    "            tok = line.split('\\t')\n",
    "\n",
    "            current_words.append(tok[1])\n",
    "            current_tags.append(tok[2])\n",
    "        else:\n",
    "            if current_words:\n",
    "                data.append((current_words, current_tags))\n",
    "            current_words = []\n",
    "            current_tags = []\n",
    "\n",
    "    if current_tags != []:\n",
    "        data.append((current_words, current_tags))\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['words', 'tags'])\n",
    "    df['id'] = df.index\n",
    "    df = df[['id', 'words', 'tags']]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "# creating the dictionary of labels and their indices\n",
    "class Vocab():\n",
    "    def __init__(self, pad_unk='<PAD>'):\n",
    "        self.pad_unk = pad_unk\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def getIdx(self, word, add=False):\n",
    "        if word is None or word == self.pad_unk:\n",
    "            return None\n",
    "        if word not in self.word2idx:\n",
    "            if add:\n",
    "                idx = len(self.idx2word)\n",
    "                self.word2idx[word] = idx\n",
    "                self.idx2word.append(word)\n",
    "                return idx\n",
    "            else:\n",
    "                return None\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def getWord(self, idx):\n",
    "        return self.idx2word[idx]\n",
    "    \n",
    "\n",
    "# tokenizing the labels\n",
    "def tokenize_and_align_labels(dataset, word_column, tag_column, tokenizer):\n",
    "    tokenized_inputs = tokenizer(dataset[word_column].tolist(), truncation=True, is_split_into_words=True, padding = True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(dataset[tag_column]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs.data\n",
    "\n",
    "\n",
    "# function for computing the metrics from a trained model\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2. Log in into the hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0c56b2c830429b8284dbc6f9c7a71d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Password for the hub (Anna)\n",
    "# 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\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3. Set up all the neccessary paths and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annam\\AppData\\Local\\Temp\\ipykernel_25272\\250303609.py:28: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"seqeval\")\n",
      "c:\\Users\\annam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\load.py:756: FutureWarning: The repository for seqeval contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/seqeval/seqeval.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# specify the datasets you want to use\n",
    "train_path = \"data\\sk_snk-ud-train.iob2\"\n",
    "dev_path = \"data\\sk_snk-ud-dev.iob2\"\n",
    "\n",
    "\n",
    "# specify the model and the tokenizer you want to use\n",
    "model_name = \"distilbert/distilbert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True)\n",
    "\n",
    "\n",
    "# This will be used to create a repository on the hubbging face\n",
    "# for saving our model and the logs (checkpoints)\n",
    "hub_folder = \"trained_slovak\"\n",
    "\n",
    "# Folder in which all the label indices will be saved\n",
    "label_folder = \"01_label_indices/labels_slovak\"\n",
    "\n",
    "\n",
    "# setting up variables for the model\n",
    "batch_size = 8\n",
    "evaluation_strategy = \"epoch\"\n",
    "learning_rate = 2e-5\n",
    "num_train_epochs = 3\n",
    "\n",
    "\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "\n",
    "# what does this do???\n",
    "label_all_tokens = True # dw about it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4. Loading and formatting the data for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# read in the data as a dataframe\n",
    "train_data = read_iob2_file(train_path)\n",
    "dev_data = read_iob2_file(dev_path)\n",
    "\n",
    "\n",
    "# get the tag dictionary\n",
    "label_indices = Vocab()\n",
    "tags_column = train_data[\"tags\"]\n",
    "\n",
    "for tags in tags_column:\n",
    "    for tag in tags:\n",
    "        label_indices.getIdx(tag, add=True)\n",
    "\n",
    "label_list = label_indices.idx2word\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# translate tags into numerical\n",
    "train_data['tag_idx'] = train_data['tags'].apply(lambda x: [label_indices.word2idx[tag] for tag in x])\n",
    "dev_data['tag_idx'] = dev_data['tags'].apply(lambda x: [label_indices.word2idx[tag] for tag in x])\n",
    "\n",
    "\n",
    "\n",
    "# tokenize the data\n",
    "tokenized_data = tokenize_and_align_labels(train_data, \"words\", \"tag_idx\", tokenizer)\n",
    "tokenized_dev_data = tokenize_and_align_labels(dev_data, \"words\", \"tag_idx\", tokenizer)\n",
    "\n",
    "\n",
    "\n",
    "# turning the data into datasetdicts, to make them compatible with the trainer (otherwise they can't be indexed)\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'id': range(len(tokenized_data['input_ids'])),\n",
    "    'input_ids': tokenized_data['input_ids'],\n",
    "    'attention_mask': tokenized_data['attention_mask'],\n",
    "    'labels': tokenized_data['labels']\n",
    "})\n",
    "\n",
    "dev_dataset = Dataset.from_dict({\n",
    "    'id': range(len(tokenized_dev_data['input_ids'])),\n",
    "    'input_ids': tokenized_dev_data['input_ids'],\n",
    "    'attention_mask': tokenized_dev_data['attention_mask'],\n",
    "    'labels': tokenized_dev_data['labels']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the file already exists\n",
    "file_path = label_folder + \".pkl\"\n",
    "if os.path.exists(file_path):\n",
    "    raise FileExistsError(f\"The file {file_path} already exists. It won't be overwritten.\")\n",
    "\n",
    "# Save the label list as a pickle\n",
    "with open(file_path, 'wb') as f:\n",
    "    pickle.dump(label_indices, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5. Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert/distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\annam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    " # verify the tokenizers compatibility with hugging face\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)\n",
    "\n",
    "#Specify waht model you want to deploy\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name, num_labels=len(label_list))\n",
    "\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir = hub_folder,\n",
    "    evaluation_strategy = evaluation_strategy,\n",
    "    learning_rate = learning_rate,\n",
    "    per_device_train_batch_size = batch_size,\n",
    "    per_device_eval_batch_size = batch_size,\n",
    "    num_train_epochs = num_train_epochs,\n",
    "    gradient_accumulation_steps = 4\n",
    "\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=dev_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e9c9f065fa64bdeb32db22236f1bb85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/795 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8975fd7640fe4b5b8b49ad165e9085c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14896905422210693, 'eval_precision': 0.5769230769230769, 'eval_recall': 0.5815508021390374, 'eval_f1': 0.5792276964047935, 'eval_accuracy': 0.9579032819204453, 'eval_runtime': 7.9116, 'eval_samples_per_second': 133.98, 'eval_steps_per_second': 16.811, 'epoch': 1.0}\n",
      "{'loss': 0.0775, 'grad_norm': 1.2549395561218262, 'learning_rate': 7.421383647798742e-06, 'epoch': 1.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31d4b1daeef4e578b8b3c37c69f00b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11313632130622864, 'eval_precision': 0.6523754345307068, 'eval_recall': 0.7526737967914439, 'eval_f1': 0.6989447548106765, 'eval_accuracy': 0.9679925779890989, 'eval_runtime': 8.513, 'eval_samples_per_second': 124.515, 'eval_steps_per_second': 15.623, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895f8e1db890415cb4014f28896f14cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11342089623212814, 'eval_precision': 0.6850393700787402, 'eval_recall': 0.7560160427807486, 'eval_f1': 0.7187797902764538, 'eval_accuracy': 0.9692295798059454, 'eval_runtime': 7.6701, 'eval_samples_per_second': 138.2, 'eval_steps_per_second': 17.34, 'epoch': 3.0}\n",
      "{'train_runtime': 596.9731, 'train_samples_per_second': 42.63, 'train_steps_per_second': 1.332, 'train_loss': 0.056915977166133856, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=795, training_loss=0.056915977166133856, metrics={'train_runtime': 596.9731, 'train_samples_per_second': 42.63, 'train_steps_per_second': 1.332, 'train_loss': 0.056915977166133856, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the trained model on the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a8ac1cfff994336bb2d34afde99d875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/539M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed8365cea1a24530a8fb5218c62a7c8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1714414184.PAMPUCH.25272.0:   0%|          | 0.00/6.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53e6c268cbf04f4eb8d196c7cbbbe3e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "events.out.tfevents.1714414097.PAMPUCH.25176.1:   0%|          | 0.00/4.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3304d75d101c4183ac95bbdfa7e98c9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2da0f457c5e4445ad26199a237e9d7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training_args.bin:   0%|          | 0.00/4.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/annamariagnat/trained_slovak/commit/5f5284035e51053a54d10aa2fb6df99558ae700e', commit_message='End of training', commit_description='', oid='5f5284035e51053a54d10aa2fb6df99558ae700e', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6. Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9cd756d9f0647a6b3baae5ef025066c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1061 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\annam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3229\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3226\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   3228\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 3229\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3230\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3232\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   3233\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   3234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   3235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3237\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3239\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\annam\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:3520\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   3516\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[0;32m   3517\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[0;32m   3518\u001b[0m         )\n\u001b[0;32m   3519\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3520\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3521\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   3522\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[3], line 93\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m     87\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     89\u001b[0m true_predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     90\u001b[0m     [label_list[p] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[0;32m     92\u001b[0m ]\n\u001b[1;32m---> 93\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[0;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     98\u001b[0m results \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mtrue_predictions, references\u001b[38;5;241m=\u001b[39mtrue_labels)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_recall\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    104\u001b[0m }\n",
      "Cell \u001b[1;32mIn[3], line 94\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     87\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     89\u001b[0m true_predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     90\u001b[0m     [label_list[p] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[0;32m     92\u001b[0m ]\n\u001b[0;32m     93\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 94\u001b[0m     \u001b[43m[\u001b[49m\u001b[43mlabel_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m!=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[0;32m     96\u001b[0m ]\n\u001b[0;32m     98\u001b[0m results \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mtrue_predictions, references\u001b[38;5;241m=\u001b[39mtrue_labels)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_recall\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    104\u001b[0m }\n",
      "Cell \u001b[1;32mIn[3], line 94\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     87\u001b[0m predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     89\u001b[0m true_predictions \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     90\u001b[0m     [label_list[p] \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[0;32m     92\u001b[0m ]\n\u001b[0;32m     93\u001b[0m true_labels \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m---> 94\u001b[0m     [\u001b[43mlabel_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m (p, l) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(prediction, label) \u001b[38;5;28;01mif\u001b[39;00m l \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m]\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prediction, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(predictions, labels)\n\u001b[0;32m     96\u001b[0m ]\n\u001b[0;32m     98\u001b[0m results \u001b[38;5;241m=\u001b[39m metric\u001b[38;5;241m.\u001b[39mcompute(predictions\u001b[38;5;241m=\u001b[39mtrue_predictions, references\u001b[38;5;241m=\u001b[39mtrue_labels)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_precision\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_recall\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m: results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverall_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    104\u001b[0m }\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68bd25014eb54111ab79f5e4914a49bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 0.7937956204379562,\n",
       "  'recall': 0.7323232323232324,\n",
       "  'f1': 0.7618213660245183,\n",
       "  'number': 594},\n",
       " 'ORG': {'precision': 0.5396825396825397,\n",
       "  'recall': 0.5811965811965812,\n",
       "  'f1': 0.5596707818930041,\n",
       "  'number': 351},\n",
       " 'PER': {'precision': 0.8125,\n",
       "  'recall': 0.8395303326810176,\n",
       "  'f1': 0.8257940327237728,\n",
       "  'number': 511},\n",
       " 'overall_precision': 0.734525447042641,\n",
       " 'overall_recall': 0.7335164835164835,\n",
       " 'overall_f1': 0.7340206185567011,\n",
       " 'overall_accuracy': 0.9772035530157153}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluating using dev data\n",
    "dev_dataset_new = Dataset.from_dict({\n",
    "    'input_ids': dev_dataset['input_ids'],\n",
    "    'attention_mask': dev_dataset['attention_mask'],\n",
    "    'labels': dev_dataset['labels']\n",
    "})\n",
    "\n",
    "predictions, labels, _ = trainer.predict(dev_dataset_new)\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
